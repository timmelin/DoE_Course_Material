{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fac9d630",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Teoroo-CMC/DoE_Course_Material/blob/main/Week_2/Workshop_4/Jupyter-notebooks/2-6factor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12a3edb8",
   "metadata": {},
   "source": [
    "# A Two-Level, Six-Factor Full Factorial Design\n",
    "\n",
    "## Introduction\n",
    "This notebook roughly follows content from Box and Draper's Empirical Model-Building and Response Surfaces (Wiley, 1984). This content is covered by Chapter 4 of Box and Draper.\n",
    "\n",
    "In this notebook, we'll carry out an anaylsis of a full factorial design, and show how we can obtain information about a system and its responses, and a quantifiable range of certainty about those values. This is the fundamental idea behind empirical model-building and allows us to construct cheap and simple models to represent complex, nonlinear systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5829c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import rand, seed\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "seed(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "876ac13c",
   "metadata": {},
   "source": [
    "## Two-Level Six-Factor Full Factorial Design\n",
    "Let's start with our six-factor factorial design example. Six factors means there are six input variables; this is still a two-level experiment, so this is now a  2$^6$-factorial experiment.\n",
    "\n",
    "Additionally, there are now three response variables,  (y$_1$,y$_2$,y$_3$)\n",
    " .\n",
    "\n",
    "To generate a table of the 64 experiments to be run at each factor level, we will use the itertools.product function below. This is all put into a DataFrame.\n",
    "\n",
    "This example generates some random response data, by multiplying a vector of random numbers by the vector of input variable values. (Nothing too complicated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418791ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Create the inputs:\n",
    "encoded_inputs = list( itertools.product([-1,1],[-1,1],[-1,1],[-1,1],[-1,1],[-1,1]) )\n",
    "\n",
    "# Create the experiment design table (same as the book):\n",
    "doe=pd.DataFrame(encoded_inputs)\n",
    "doe=doe[doe.columns[::-1]]\n",
    "doe.columns=['x%d'%(i+1) for i in range(6)]\n",
    "doe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bff432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y-data from book.\n",
    "doe['y1'] = [3.4, 9.7, 7.4, 10.6, 6.5, 7.9, 10.3, 9.5, 14.3, 10.5, 7.8, 17.2, 9.4, 12.1, 9.5, 15.8, 8.3, 8.0, 7.9, 10.7, 7.2, 7.2, 7.9, 10.2, 10.3, 9.9, 7.4, 10.5, 9.6, 15.1, 8.7, 12.1, 12.6, 10.5, 11.3, 10.6, 8.1, 12.5, 11.1, 12.9, 14.6, 12.7, 10.8, 17.1, 13.6, 14.6, 13.3, 14.4, 11.0, 12.5, 8.9, 13.1, 7.6, 8.6, 11.8, 12.4, 13.4, 14.6, 14.9, 11.8, 15.6, 12.8, 13.5, 15.8] \n",
    "doe['y2'] = [15,5,23, 8,20, 9, 13, 5, 23, 1, 11, 5, 15, 8, 15, 1, 22, 8, 16, 7, 25, 5, 17, 8, 10, 3, 22, 6, 24, 4, 10, 5, 32, 10, 28, 18, 22, 31, 17, 16, 38, 12, 34, 19, 12, 14, 25, 16, 31, 14, 23, 23, 28, 20, 18, 11, 39, 30, 31, 6, 33, 23, 31, 11]\n",
    "doe['y3'] = [36, 35, 37, 34, 30, 32, 28, 38, 40, 32, 32, 28, 34, 26,  30, 28, 40, 30, 35, 35, 32, 35, 36, 32, 20, 35, 35, 28, 27, 36, 36, 35, 32, 34, 30, 24, 30, 20, 32, 25, 20, 20, 22, 35, 26, 15, 19, 24, 22, 23, 22, 18, 20, 20, 20, 36, 20, 11, 20, 35, 16, 32, 20, 20]\n",
    "print(doe[['y1','y2','y3']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5bf3143",
   "metadata": {},
   "source": [
    "## Defining Variables and Variable Labels\n",
    "Next we'll define some containers for input variable labels, output variable labels, and any interaction terms that we'll be computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350898a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "labels[1] = ['x1','x2','x3','x4','x5','x6']\n",
    "for i in [2,3,4,5,6]:\n",
    "    labels[i] = list(itertools.combinations(labels[1], i))\n",
    "\n",
    "obs_list = ['y1','y2','y3']\n",
    "\n",
    "for k in labels.keys():\n",
    "    print(str(k) + \" : \" + str(labels[k]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4abb556e",
   "metadata": {},
   "source": [
    "Now that we have variable labels for each main effect and interaction effect, we can actually compute those effects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "628b3b50",
   "metadata": {},
   "source": [
    "## Computing Main and Interaction Effects\n",
    "We'll start by finding the constant effect, which is the mean of each response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1113763",
   "metadata": {},
   "outputs": [],
   "source": [
    "effects = {}\n",
    "\n",
    "# Start with the constant effect: this is $\\overline{y}$\n",
    "effects[0] = {'x0' : [doe['y1'].mean(),doe['y2'].mean(),doe['y3'].mean()]}\n",
    "print(effects[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47a35373",
   "metadata": {},
   "source": [
    "Next, compute the main effect of each variable, which quantifies the amount the response changes by when the input variable is changed from the -1 to +1 level. That is, it computes the average effect of an input variable  x$_i$\n",
    "  on each of the three response variables  y$_1$,y$_2$,y$_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aec4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "effects[1] = {}\n",
    "for key in labels[1]:\n",
    "    effects_result = []\n",
    "    for obs in obs_list:\n",
    "        effects_df = doe.groupby([key])[obs].mean()\n",
    "        result = sum([ zz*effects_df.loc[zz] for zz in effects_df.index ])\n",
    "        effects_result.append(result)\n",
    "    effects[1][key] = effects_result\n",
    "\n",
    "effects[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82363a9d",
   "metadata": {},
   "source": [
    "Our next step is to crank through each variable interaction level: two-variable, three-variable, and on up to six-variable interaction effects. We compute interaction effects for each two-variable combination, three-variable combination, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9220cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [2,3,4,5,6]:\n",
    "    effects[c] = {}\n",
    "    for key in labels[c]:\n",
    "        effects_result = []\n",
    "        for obs in obs_list:\n",
    "            if c==2:\n",
    "                effects_df = doe.groupby([key[0],key[1]])[obs].mean()\n",
    "            if c==3: \n",
    "                effects_df = doe.groupby([key[0],key[1],key[2]])[obs].mean()  \n",
    "            if c==4: \n",
    "                effects_df = doe.groupby([key[0],key[1],key[2],key[3]])[obs].mean() \n",
    "            if c==5: \n",
    "                effects_df = doe.groupby([key[0],key[1],key[2],key[3],key[4]])[obs].mean()\n",
    "            if c==6: \n",
    "                effects_df = doe.groupby([key[0],key[1],key[2],key[3],key[4],key[5]])[obs].mean()    \n",
    "            result = sum([ np.prod(zz)*effects_df.loc[zz]/(2**(len(zz)-1)) for zz in effects_df.index ])\n",
    "            effects_result.append(result)\n",
    "        effects[c][key] = effects_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb030c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printd(d):\n",
    "    for k in d.keys():\n",
    "        print(\"%25s : %s\"%(k,d[k]))\n",
    "\n",
    "for i in range(1,7):\n",
    "    printd(effects[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f83d24e1",
   "metadata": {},
   "source": [
    "We've computed the main and interaction effects for every variable combination (whew!), but now we're at a point where we want to start doing things with these quantities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53f505f8",
   "metadata": {},
   "source": [
    "# Analyzing Effects\n",
    "The first and most important question is, what variable, or combination of variables, has the strongest effect on the three responses  y$_1$?  y$_2$? y$_3$?\n",
    "\n",
    "To figure this out, we'll need to use the data we computed above. Python makes it easy to slice and dice data. In this case, we've constructed a nested dictionary, with the outer keys mapping to the number of variables and inner keys mapping to particular combinations of input variables. Its pretty easy to convert this to a flat data structure that we can use to sort by variable effects. We've got six \"levels\" of variable combinations, so we'll flatten effects by looping through all six dictionaries of variable combinations (from main effects to six-variable interaction effects), and adding each entry to a master dictionary.\n",
    "\n",
    "The master dictionary will be a flat dictionary, and once we've populated it, we can use it to make a DataFrame for easier sorting, printing, manipulating, aggregating, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(effects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8740c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict = {}\n",
    "for nvars in effects.keys():\n",
    "\n",
    "    effect = effects[nvars]\n",
    "    for k in effect.keys():\n",
    "        v = effect[k]\n",
    "        master_dict[k] = v\n",
    "\n",
    "master_df = pd.DataFrame(master_dict).T\n",
    "master_df.columns = obs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd53214",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = master_df['y1'].copy()\n",
    "y1.sort_values(inplace=True,ascending=False)\n",
    "\n",
    "#df.reindex(df.b.abs().sort_values().index)\n",
    "\n",
    "print(\"Top 10 effects for observable y1:\")\n",
    "print(y1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = master_df['y2'].copy()\n",
    "y2.sort_values(inplace=True,ascending=False)\n",
    "\n",
    "print(\"Top 10 effects for observable y2:\")\n",
    "print(y2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee01ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = master_df['y3'].copy()\n",
    "y3.sort_values(inplace=True,ascending=False)\n",
    "\n",
    "print(\"Top 10 effects for observable y3:\")\n",
    "print(y3[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc998800",
   "metadata": {},
   "source": [
    "If we were only to look at the list of rankings of each variable, we would see that each response is affected by different input variables, listed in order of descending importance from positive to negative... FIX!!!  \n",
    "\n",
    "Let us have a look at Pareto charts instead... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10ae77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "master_df\n",
    "indexes=['y1', 'y2', 'y3']\n",
    "for ii in indexes: \n",
    "    effects_df=pd.DataFrame(master_df[ii][1:-1].abs())\n",
    "    effects_df = effects_df.sort_values(by=ii, ascending=False)\n",
    "# Add cumulative percentage column\n",
    "    effects_df[\"cum_percentage\"] = round(effects_df[ii].cumsum()/effects_df[ii].sum()*100,2)\n",
    "    plt.rcParams.update({'font.size': 22})\n",
    "# Set figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(22,10))\n",
    "\n",
    "# Plot bars (i.e. frequencies)\n",
    "    ax.set_title(\"Pareto Chart\")\n",
    "    ax.set_xlabel(\"Parameter\")\n",
    "    ax.set_ylabel(\"Frequency\");\n",
    "    effects_df[ii].plot.bar(y='Standardized effect', ax=ax)\n",
    "#    ax.axhline(2.06, color=\"orange\", linestyle=\"dashed\")\n",
    "\n",
    "# Second y axis (i.e. cumulative percentage)\n",
    "    ax2 = ax.twinx()\n",
    "#ax2.plot(effects_df.index, effects_df[\"cum_percentage\"], color=\"red\", marker=\"D\", ms=7)\n",
    "    effects_df.plot(y=\"cum_percentage\", color=\"red\", marker=\"D\", ms=7, ax=ax2)\n",
    "    ax2.yaxis.set_major_formatter(PercentFormatter())\n",
    "    ax2.set_ylabel(\"Cumulative Percentage\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30d55902",
   "metadata": {},
   "source": [
    "However, looking at the quantile-quantile plot of the effects answers the question in a more visual way.\n",
    "\n",
    "## Quantile-Quantile Effects Plot\n",
    "We can examine the distribution of the various input variable effects using a quantile-quantile plot of the effects. Quantile-quantile plots arrange the effects in order from least to greatest, and can be applied in several contexts (as we'll see below, when assessing model fits). If the quantities plotted on a quantile-qantile plot are normally distributed, they will fall on a straight line; data that do not fall on the straight line indicate significant deviations from normal behavior.\n",
    "\n",
    "In the case of a quantile-quantile plot of effects, non-normal behavior means the effect is paticularly strong. By identifying the outlier points on thse quantile-quantile plots (they're ranked in order, so they correspond to the lists printed above), we can identify the input variables most likely to have a strong impact on the responses.\n",
    "\n",
    "We need to look both at the top (the variables that have the largest overall positive effect) and the bottom (the variables that have the largest overall negative effect) for significant outliers. When we find outliers, we can add them to a list of variabls that we have decided are important and will keep in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ff7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify which effects are not normally distributed, \n",
    "# to assist in identifying important variables\n",
    "\n",
    "fig = figure(figsize=(14,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "stats.probplot(y1[1:-1], dist=\"norm\", plot=ax1)\n",
    "ax1.set_title('y1')\n",
    "\n",
    "stats.probplot(y2[1:-1], dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('y2')\n",
    "\n",
    "stats.probplot(y3[1:-1], dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('y3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be91ef43",
   "metadata": {},
   "source": [
    "What conclusion can we draw from this analysis? What does the book say? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
