{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adf728c3",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Teoroo-CMC/DoE_Course_Material/blob/main/Week_2/Workshop_3/Jupyter-notebooks/DoE-2factor_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import rand"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f7b0d5e",
   "metadata": {},
   "source": [
    "# Design of experiments \n",
    "\n",
    "\n",
    "\n",
    "## Inputs and Responses\n",
    "In this example, we investigate a problem often encoundered in engineering. A product is produced using a specific infill, orientation and with a nozzle of a certain diameter. All taken together, the settings give rize to a response which becomes a measure of the quality of the product. A higher response results in higher quality. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d8ecb9a",
   "metadata": {},
   "source": [
    "## Two-level full factorial design\n",
    "To find which parameters that will have most influence on the quality, we need to perform experiments. For this purpose, lets consider a a two-level, three-variable experimental design - also written as 2$^3$, with n=2 levels for each factor,  k=3 different factors. We start by encoding each fo the three variables to something generic: (x1,x2,x3). A dataframe with input variable values is then populated.\n",
    "\n",
    "In this design, each observation data point consists of three input variable values and an output variable value, (A,B,C,y), and can be thought of as a point in 3D space (A,B,C) with an associated point value of y. Alternatively, this might be thought of as a point in 4D space (the first three dimensions are the location in 3D space where the point will appear, and the y value is when it will actually appear).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e1b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs_labels = {'A' : 'Infill',\n",
    "                 'B' : 'Orientation',\n",
    "                 'C' : 'Nozzle Diameter'}\n",
    "\n",
    "dat = [('A',5,100),\n",
    "       ('B',0,90),\n",
    "       ('C',0.2,1)]\n",
    "\n",
    "inputs_df = pd.DataFrame(dat,columns=['index','low','high'])\n",
    "inputs_df = inputs_df.set_index(['index'])\n",
    "inputs_df['label'] = inputs_df.index.map( lambda z : inputs_labels[z] )\n",
    "\n",
    "inputs_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c30c474",
   "metadata": {},
   "source": [
    "We encode the variable values. For an arbitrary variable value  $\\phi_i$, the value of the variable can be coded to be between -1 and 1 according to the formula:\n",
    "\n",
    "\\begin{equation}\n",
    "x_i = \\frac{\\phi_i-avg(\\phi)}{span(\\phi)}\n",
    "\\end{equation}\n",
    "\n",
    "where the average and the span of the variable $\\phi_i$ are defined as: \n",
    "\n",
    "\\begin{equation}\n",
    "avg(\\phi) = \\frac{\\phi_{high}+\\phi_{low}}{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "span(\\phi) = \\frac{\\phi_{high}-\\phi_{low}}{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_df['average'] = inputs_df.apply( lambda z : ( z['high'] + z['low'])/2 , axis=1)\n",
    "inputs_df['span'] = inputs_df.apply( lambda z : ( z['high'] - z['low'])/2 , axis=1)\n",
    "\n",
    "inputs_df['encoded_low'] = inputs_df.apply( lambda z : ( z['low']  - z['average'] )/( z['span'] ), axis=1)\n",
    "inputs_df['encoded_high'] = inputs_df.apply( lambda z : ( z['high'] - z['average'] )/( z['span'] ), axis=1)\n",
    "\n",
    "inputs_df = inputs_df.drop(['average','span'],axis=1)\n",
    "\n",
    "inputs_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a7829c1",
   "metadata": {},
   "source": [
    "The input variable values consist of all possible input value combinations, which we can produce using the itertools module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fdd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# we have four repetitions\n",
    "encoded_inputs= list(itertools.product([-1,1],[-1,1],[-1,1]))\n",
    "encoded_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa5fe5e0",
   "metadata": {},
   "source": [
    "Now we implement the observed outcomes (4 repetitions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94814dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions=4\n",
    "y1= [8,26.2,14.8,44,16.6,28.2,14.8,42.2] \n",
    "y2= [9.2,33.6,15.2,44,12.4,24.4,15,43.2]\n",
    "y3= [14.2,40,16,45,12.8,24.4,14.6,41.8]\n",
    "y4= [10.6,28.8,15.2,43.8,14,23.2,20.8,41]\n",
    "y=[8,26.2,14.8,44,16.6,28.2,14.8,42.2,9.2,33.6,15.2,44,12.4,24.4,15,43.2,14.2,40,16,45,12.8,24.4,14.6,41.8,10.6,28.8,15.2,43.8,14,23.2,20.8,41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8cd1b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results=pd.DataFrame(encoded_inputs,columns=['A','B','C'])\n",
    "results['y1']=y1\n",
    "results['y2']=y2\n",
    "results['y3']=y3\n",
    "results['y4']=y4\n",
    "results['ybar']= results.iloc[:, 3:7].mean(axis=1)\n",
    "results "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "573703e8",
   "metadata": {},
   "source": [
    "The variable inputs_df contains all input variables for the expeirment design, and results_df contains the inputs and responses for the experiment design; these variables are the encoded levels. To obtain the original, unscaled values, which allows us to check what experiments must be run, we can always convert the dataframe back to its originals by defining a function to un-apply the scaling equation. This is as simple as finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5868a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_experiment = results\n",
    "\n",
    "var_labels = []\n",
    "for var in ['A','B','C']:\n",
    "    var_label = inputs_df.loc[var]['label']\n",
    "    var_labels.append(var_label)\n",
    "    real_experiment[var_label] = results.apply(\n",
    "        lambda z : inputs_df.loc[var]['low'] if z[var]<0 else inputs_df.loc[var]['high'] , \n",
    "        axis=1)\n",
    "\n",
    "print(\"The values of each real variable in the experiment:\")\n",
    "real_experiment[var_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "# plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(real_experiment.Infill,\n",
    "             real_experiment.Orientation,\n",
    "             real_experiment['Nozzle Diameter'],\n",
    "           s=200)\n",
    "ax.set_xlabel('Infill')\n",
    "ax.set_ylabel('Orientation')\n",
    "ax.set_zlabel('Nozzle Diameter');\n",
    "ax.view_init(30, 160)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b6e1aaf",
   "metadata": {},
   "source": [
    "# Computing main effects\n",
    "\n",
    "Now we compute the main effects of each variable using the results of the experimental design. We'll use some shorthand Pandas functions to compute these averages: the groupby function, which groups rows of a dataframe according to some condition (in this case, the value of our variable of interest  x$_i$ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean effect of the factor on the response,\n",
    "# conditioned on each variable\n",
    "labels = ['A','B','C']\n",
    "\n",
    "main_effects = {}\n",
    "rep=4\n",
    "for j in range(0,rep):\n",
    "    index=j+1\n",
    "    for key in labels:\n",
    "        effects = results.groupby(key)['y'+str(index)].mean()\n",
    "        main_effects[j,key] = sum( [i*effects[i] for i in [-1,1]])\n",
    "        \n",
    "print('Main effects')    \n",
    "print(main_effects)\n",
    "\n",
    "average_main_effects = {}\n",
    "\n",
    "print('Average main effects')\n",
    "for key in labels:\n",
    "        average_effects = results.groupby(key)['ybar'].mean()\n",
    "        average_main_effects[key] = sum( [i*average_effects[i] for i in [-1,1]])\n",
    "print(average_main_effects)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d83a00b5",
   "metadata": {},
   "source": [
    "# Analyzing Main Effects\n",
    "The main effect of a given variable (as defined by Yates 1937) is the average difference in the level of response as the input variable moves from the low to the high level. If there are other variables, the change in the level of response is averaged over all combinations of the other variables.\n",
    "\n",
    "Now that we've computed the main effects, we can analyze the results to glean some meaningful information about our system. The first variable A has a negative effect of -1.65 - this indicates that when A goes from its low level to its high level, it decreases the value of the response. This means A should be increased, if we want an increased response. However, this effect was the smallest, meaning it's might have a very small effect on the response.\n",
    "\n",
    "This might be the case if, for example, changing the value of the input variables were capital-intensive. A company might decide that they can only afford to change one variable, A, B, or C. If this were the case, increasing A would not be the way to go.\n",
    "\n",
    "In contrast, increasing the variables B and C will result in an increase in the response, since these have a positive main effect. These variables should be kept at their higher levels, or increased, to increase the response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a2128fb",
   "metadata": {},
   "source": [
    "## Two-Way Interactions\n",
    "In addition to main effects, a factorial design will also reveal interaction effects between variables - both two-way interactions and three-way interactions. We can use the itertools library to compute the interaction effects using the results from the factorial design.\n",
    "\n",
    "We'll use the Pandas groupby function again, grouping by two variables this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "twoway_labels = list(itertools.combinations(labels, 2))\n",
    "\n",
    "\n",
    "twoway_effects = {}\n",
    "for key in twoway_labels:\n",
    "    \n",
    "    effects = results.groupby([key[0],key[1]])['ybar'].mean()\n",
    "    \n",
    "    twoway_effects[key] = sum([ i*j*effects[i][j]/2 for i in [-1,1] for j in [-1,1] ])\n",
    "twoway_effects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65a2ecc6",
   "metadata": {},
   "source": [
    "This one-liner is a bit hairy:\n",
    "\n",
    "```\n",
    "twoway_effects[key] = sum([ i*j*effects[i][j]/2 for i in [-1,1] for j in [-1,1] ])\n",
    "```\n",
    "\n",
    "What this does is, computes the two-way variable effect with a multi-step calculation, but does it with a list comprehension. \n",
    "\n",
    "First, let's just look at this part:\n",
    "\n",
    "```\n",
    "i*j*effects[i][j]/2 for i in [-1,1] for j in [-1,1]\n",
    "```\n",
    "This computes the prefix i*j, which determines if the interaction effect effects[i][j] is positive or negative. We're also looping over one additional dimension; we multiply by 1/2 for each additional dimension we loop over. These are all summed up to yield the final interaction effect for every combination of the input variables.\n",
    "\n",
    "If we were computing three-way interaction effects, we would have a similar-looking one-liner, but with i, j, and k:\n",
    "```\n",
    "i*j*k*effects[i][j][k]/4 for i in [-1,1] for j in [-1,1] for k in [-1,1]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7834bc6a",
   "metadata": {},
   "source": [
    "## Analyzing Two-Way Interactions\n",
    "\n",
    "As with main effects, we can analyze the results of the interaction effects analysis to come to some useful conclusions about our physical system. A two-way interaction is a measure of how the main effect of one variable changes as the level of another variable changes. A negative two-way interaction between A and C means that if we increase  C, the main effect of A will be to decrease the response; or, alternatively, if we increase A, the main effect of C will be to decrease the response.\n",
    "\n",
    "In this case, we see that the B−C interaction effect is the largest, and it is positive. This means that if we increase both B and C, it will increase our response - make the equipment last longer. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d13f0e30",
   "metadata": {},
   "source": [
    "## Three-Way Interactions\n",
    "Now let's compute the three-way effects (in this case, we can only have one three-way effect, since we only have three variables). We'll start by using the itertools library again, to create a tuple listing the three variables whose interactions we're computing. Then we'll use the Pandas groupby() feature to partition each output according to its inputs, and use it to compute the three-way effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "threeway_labels = list(itertools.combinations(labels, 3))\n",
    "\n",
    "threeway_effects = {}\n",
    "for key in threeway_labels:\n",
    "    \n",
    "    effects = results.groupby([key[0],key[1],key[2]])['ybar'].mean()\n",
    "    \n",
    "    threeway_effects[key] = sum([ i*j*k*effects[i][j][k]/4 for i in [-1,1] for j in [-1,1] for k in [-1,1] ])\n",
    "\n",
    "threeway_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "effects=[] #pd.DataFrame({})\n",
    "indexes=[]\n",
    "for i,k in enumerate(average_main_effects.keys()):\n",
    "    effects.append(abs(average_main_effects[k]))\n",
    "    indexes.append(k)\n",
    "for i,k in enumerate(twoway_effects.keys()):\n",
    "    effects.append(abs(twoway_effects[k]))\n",
    "    indexes.append(k)\n",
    "for i,k in enumerate(threeway_effects.keys()):\n",
    "    effects.append(abs(threeway_effects[k]))\n",
    "    indexes.append(k)    \n",
    "    \n",
    "effects_df=pd.DataFrame({\"Standardized effect\":effects})\n",
    "\n",
    "# reset the indexes\n",
    "effects_df.index=indexes\n",
    "# Sort values in descending order\n",
    "effects_df = effects_df.sort_values(by='Standardized effect', ascending=False)\n",
    "# Add cumulative percentage column\n",
    "effects_df[\"cum_percentage\"] = round(effects_df[\"Standardized effect\"].cumsum()/effects_df[\"Standardized effect\"].sum()*100,2)\n",
    "\n",
    "# Display data frame\n",
    "effects_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c1a004b",
   "metadata": {},
   "source": [
    "## Analysis of Three-Way Effects\n",
    "While three-way interactions are relatively rare, typically smaller, and harder to interpret, a negative three-way interaction esssentially means that increasing these variables, all together, will lead to interactions which lower the response (the lifespan of the equipment), and the opposite for positive values. This is esily visualized in a Pareto plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a12b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "# Set figure and axis\n",
    "fig, ax = plt.subplots(figsize=(22,10))\n",
    "\n",
    "# Plot bars (i.e. frequencies)\n",
    "ax.set_title(\"Pareto Chart\")\n",
    "ax.set_xlabel(\"Parameter\")\n",
    "ax.set_ylabel(\"Frequency\");\n",
    "effects_df.plot.bar(y='Standardized effect', ax=ax)\n",
    "ax.axhline(2.06, color=\"orange\", linestyle=\"dashed\")\n",
    "\n",
    "# Second y axis (i.e. cumulative percentage)\n",
    "ax2 = ax.twinx()\n",
    "#ax2.plot(effects_df.index, effects_df[\"cum_percentage\"], color=\"red\", marker=\"D\", ms=7)\n",
    "effects_df.plot(y=\"cum_percentage\", color=\"red\", marker=\"D\", ms=7, ax=ax2)\n",
    "ax2.yaxis.set_major_formatter(PercentFormatter())\n",
    "ax2.set_ylabel(\"Cumulative Percentage\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3174a44",
   "metadata": {},
   "source": [
    "However, looking at the quantile-quantile plot of the effects answers the question in a more visual way.\n",
    "\n",
    "## Quantile-Quantile Effects Plot\n",
    "We can examine the distribution of the various input variable effects using a quantile-quantile plot of the effects. Quantile-quantile plots arrange the effects in order from least to greatest, and can be applied in several contexts (as we'll see below, when assessing model fits). If the quantities plotted on a quantile-qantile plot are normally distributed, they will fall on a straight line; data that do not fall on the straight line indicate significant deviations from normal behavior.\n",
    "\n",
    "In the case of a quantile-quantile plot of effects, non-normal behavior means the effect is paticularly strong. By identifying the outlier points on thse quantile-quantile plots (they're ranked in order, so they correspond to the lists printed above), we can identify the input variables most likely to have a strong impact on the responses.\n",
    "\n",
    "We need to look both at the top (the variables that have the largest overall positive effect) and the bottom (the variables that have the largest overall negative effect) for significant outliers. When we find outliers, we can add them to a list of variabls that we have decided are important and will keep in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddfd167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import *\n",
    "import scipy.stats as stats\n",
    "fig = figure(figsize=(14,4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "#ax2 = fig.add_subplot(132)\n",
    "#ax3 = fig.add_subplot(133)\n",
    "\n",
    "stats.probplot(effects_df[\"Standardized effect\"], dist=\"norm\", plot=ax1)\n",
    "ax1.set_title('y1')\n",
    "\n",
    "#stats.probplot(y2, dist=\"norm\", plot=ax2)\n",
    "#ax2.set_title('y2')\n",
    "\n",
    "#stats.probplot(y3, dist=\"norm\", plot=ax3)\n",
    "#ax3.set_title('y3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74c4edac",
   "metadata": {},
   "source": [
    "# Fitting a Polynomial Response Surface\n",
    "While identifying general trends and the effects of different input variables on a system response is useful, it's more useful to have a mathematical model for the system. The factorial design we used is designed to get us coefficients for a linear model  $\\hat{y}$ that is a linear function of input variables x$_i$, and that predicts the actual system response  y:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}=a_0+a_1x_1+a_2x_2+a_3x_3+a_{12}x_1x_2+a_{13}x_1x_3+a_{23}x_2x_3+a_{123}x_1x_2x_3\n",
    "\\end{equation} \n",
    "\n",
    "To determine these coefficients, we can obtain the effects we computed above. When we computed effects, we defined them as measuring the difference in the system response that changing a variable from -1 to +1 would have. Because this quantifies the change per two units of x, and the coefficients of a polynomial quantify the change per one unit of x, the effect must be divided by two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc6ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"yhat = \"\n",
    "\n",
    "s += \"%0.3f \"%(results['ybar'].mean())\n",
    "\n",
    "for i,k in enumerate(average_main_effects.keys()):\n",
    "    if(average_main_effects[k]<0):\n",
    "        s += \"%0.3f %s \"%( average_main_effects[k]/2.0, k )\n",
    "    else:\n",
    "        s += \"+ %0.3f %s \"%( average_main_effects[k]/2.0, k )\n",
    "\n",
    "for i,k in enumerate(twoway_effects.keys()):\n",
    "    if(twoway_effects[k]<0):\n",
    "        s += \" %0.3f %s %s\"%( twoway_effects[k]/2.0, k[0],k[1])\n",
    "    else:\n",
    "        s += \"+ %0.3f %s %s\"%( twoway_effects[k]/2.0, k[0],k[1])\n",
    "\n",
    "for i,k in enumerate(threeway_effects.keys()):\n",
    "    if(threeway_effects[k]<0):\n",
    "        s += \" %0.3f %s %s %s\"%( threeway_effects[k]/2.0, k[0],k[1], k[2])\n",
    "    else:\n",
    "        s += \"+ %0.3f %s %s %s\"%( threeway_effects[k]/2.0, k[0],k[1], k[2])\n",
    "\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c26c3df0",
   "metadata": {},
   "source": [
    "# The Impact of Uncertainty\n",
    "The main and interaction effects give us a more quantitative idea of what variables are important, yes. They can also be important for identifying where a model can be improved (if an input is linked strongly to a system response, more effort should be spent understanding the nature of the relationship).\n",
    "\n",
    "But there are still some practical considerations missing from the implementation above. Specifically, in the real world it is impossible to know the system repsonse, y, perfectly. Rather, we may measure the response with an instrument whose uncertainty has been quantified, or we may measure a quantity multiple times (or both). How do we determine the impact of that uncertainty on the model?\n",
    "\n",
    "Ultimately, factorial designs are based on the underlying assumption that the response y is a linear function of the inputs  x$_i$. Thus, for the three-factor full factorial experiment design, we are collecting data and running experiments in such a way that we obtain a model $\\hat{y}$ for our system response y, and  $\\hat{y}$ is a linear function of each factor:\n",
    "\n",
    "\\begin{equation}\n",
    "y^=a0+a1x1+a2x2+a3x3\n",
    "\\end{equation}\n",
    "\n",
    "The experiment design allows us to obtain a value for each coefficient a0, a1, etc. that will fit $\\hat{y}$ to y to the best of its abilities.\n",
    "\n",
    "Thus, uncertainty in the measured responses y propagates into the linear model in the form of uncertainty in the coefficients  a0, a1, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de21b1c7",
   "metadata": {},
   "source": [
    "## Uncertainty Numbers\n",
    "To obtain an estimate of the uncertainty, the experimentalist will typically make several measurements at the center point, that is, where all parameter levels are 0. The more samples are taken at this condition, the better characterized the distribution of uncertainty becomes. These center point samples can be used to construct a Gaussian probability distribution function, which yeilds a variance,  $\\sigma^2$ (or, to be proper, an estimate  s^2  of the real variance  $\\sigma^2$). This parameter is key for quantifying uncertainty."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4c91ada",
   "metadata": {},
   "source": [
    "## Using Uncertainty Measurements\n",
    "Suppose we measure  $s^2$ = 0.0050. Now what?\n",
    "\n",
    "Now we can obtain the variance of all measurements, and the variance in the effects that we computed above. These are computed via:\n",
    "\n",
    "\\begin{equation}\n",
    "Var_{mean} = V(\\bar{y}) = \\frac{\\sigma^2}{2^k}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Var_{effect}=\\frac{4\\sigma^2}{2^k}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0671935",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmasquared = 0.0050\n",
    "k = len(inputs_df.index)\n",
    "Vmean = (sigmasquared)/(2**k)\n",
    "Veffect = (4*sigmasquared)/(2**k)\n",
    "print(\"Variance in mean: %0.6f\"%(Vmean))\n",
    "print(\"Variance in effects: %0.6f\"%(Veffect))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ad6d569",
   "metadata": {},
   "source": [
    "Alternatively, if the responses y are actually averages of a given number r of  y-observations,  $\\bar{y}, then the variance will shrink:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Var_{mean} = V(\\bar{y}) = \\frac{\\sigma^2}{r2^k}\n",
    "\\end{equation}\n",
    "​\n",
    "\\begin{equation}\n",
    "Var_{effect}=\\frac{4\\sigma^2}{r2^k}\n",
    "\\end{equation}\n",
    "​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmasquared = 0.0050\n",
    "r=4\n",
    "k = len(inputs_df.index)\n",
    "Vmean = (sigmasquared)/(r*2**k)\n",
    "Veffect = (4*sigmasquared)/(r*2**k)\n",
    "print(\"Variance in mean: %0.6f\"%(Vmean))\n",
    "print(\"Variance in effects: %0.6f\"%(Veffect))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73f0c96f",
   "metadata": {},
   "source": [
    "The variance gives us an estimate of $\\sigma^2$, and if we have sigma squared we can obtain sigma. Sigma is the quantity that represents the range of response values that captures 1 sigma, or 66\\%, of the probable values of y with  $\\hat{y}. Adding a plus or minus sigma means we are capturing 2 sigma, or 95\\%, of the probable values ofy.\n",
    "\n",
    "Taking the square root of the variance gives $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(Vmean))\n",
    "print(np.sqrt(Veffect))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aaa4deeb",
   "metadata": {},
   "source": [
    "## Accounting for Uncertainty in Model\n",
    "Now we can convert the values of the effects, and the values of  $\\sigma$, to values for the final linear model:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}=a0+a1x1+a2x2+a3x3+a12x1x2+a13x1x3+a23x2x3+a123x1x2x3\n",
    "\\end{equation} \n",
    "\n",
    "We begin with the case where each variable value is at its middle point (all non-constant terms are 0), and\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}=a0\n",
    "\\end{equation} \n",
    " \n",
    "In this case, the standard error is  ±σ\n",
    "  as computed for the mean (or overall) system response,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}=a_0 \\pm \\sigma_{mean}\n",
    "\\end{equation} \n",
    " \n",
    "where  \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{mean}=\\sqrt{Var_{mean}}\n",
    "\\end{equation} \n",
    " .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e631429",
   "metadata": {},
   "outputs": [],
   "source": [
    "unc_a_0 = np.sqrt(Vmean)\n",
    "print(unc_a_0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e12101",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "At this point, we would usually dive deeper into the details of the actual problem of interest. By trying the empirical model to the system, we can draw conclusions about the physical system - for example, if we were analyzing a chemically reacting process, and we found the response to be particularly sensitive to temperature, it would indicate that the chemical reaction is sensitive to temperature, and that the reaction should be studied more deeply (in isolation from the more complicated system) to better understand the impact of temperature on the response.\n",
    "\n",
    "It's also valuable to explore the linear model that we obtained more deeply, by looking at contours of the response surface, taking first derivatives, and optimizing the input variable values to maximize or minimize the response value. We'll leave those tasks for later, and illustrate them in later notebooks.\n",
    "\n",
    "At this point we have accomplished the goal of illustrating the design, execution, and analysis of a two-level, three-factor full factorial experimental design, so we'll leave things at that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86c8ef61",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we've covered a 2-level, three-factor factorial design from start to finish, including incorporation of uncertainty information. The design of the experiment was made simple by using the itertools and pandas libraries, and we showed how to transform variables to have low and high levels, as well as demonstrating a system response transformation. The results were analyzed to obtain a linear polynomial model.\n",
    "\n",
    "However, this process was a bit cumbersome. What we'll see in later notebooks is that we can use Python modules designed for statistical modeling to fit linear models to data using least squares and regression, and carry the analysis further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
