{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into the train/validation/test dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to split your full dataset into train/validation/test datasets, and reliably use the same datasets for your modeling tasks later.\n",
    "\n",
    "Using different train/validation/test splits can dramatically affect your model performance (as seen here by the variance in $r^2$ scores for 30 models which have been trained on 30 different dataset splits) [1]:\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/Fig1_30_splits.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "[1]: C. Clement, S. K. Kauwe, T. D. Sparks, Benchmark AFLOW Data Sets for Machine Learning, figshare 2020, DOI: [10.6084/m9.figshare.11954742](https://dx.doi.org/10.6084/m9.figshare.11954742)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set a random seed to ensure reproducibility across runs\n",
    "RNG_SEED = 42\n",
    "np.random.seed(seed=RNG_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-processed dataset\n",
    "\n",
    "We will start with the processed dataset that we saved from the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "data_path = os.path.join(PATH, 'data/cp_data_cleaned.csv')\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f'Full DataFrame shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the DataFrame into your input variables ($X$) and target variables ($y$)\n",
    "\n",
    "The $X$ will be used as the input data, and $y$ will be used as the prediction targets for your ML model.\n",
    "\n",
    "If your target variables are discrete (such as `metal`/`non-metal` or types of crystal structures), then you will be performing a classification task.\n",
    "In our case, since our target variables are continuous values (heat capacity), we are performing a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['formula', 'T']]\n",
    "y = df['Cp']\n",
    "\n",
    "print(f'Shape of X: {X.shape}')\n",
    "print(f'Shape of y: {y.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data (and a word of caution)\n",
    "### Normally, we could simply split the data with a simple `sklearn` function\n",
    "\n",
    "The scikit-learn `train_test_split` function randomly splits a dataset into train and test datasets.\n",
    "Typically, you can use `train_test_split` to first split your data into \"train\" and \"test\" datasets, and then use the function again to split your \"train\" data into \"train\" and \"validation\" dataset splits.\n",
    "\n",
    "As a rule of thumb, you can roughly aim for the following dataset proportions when splitting your data:\n",
    "\n",
    "| | train split | validation split | test split |\n",
    "| --- | --- | --- | --- |\n",
    "| proportion<br> of original<br> dataset | 50% to 70% | 20% to 30% | 10% to 20% |\n",
    "\n",
    "If you have copious amounts of data, it may suffice to train your models on just 50% of the data; that way, you have a larger amount of data samples to validate and to test with.\n",
    "If you however have a smaller dataset and thus very few training samples for your models, you may wish to increase your proportion of training data during dataset splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RNG_SEED)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But wait, what's wrong here?\n",
    "\n",
    "We have to make sure that our dataset splits contain mutually exclusive formulae (e.g., all the data samples associated with \"Al2O3\" is *either* in the train, validation, or test dataset, but *not in multiple*)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(X_train)\n",
    "print(f'There are in total {num_rows} rows in the X_train DataFrame.')\n",
    "\n",
    "num_unique_formulae = len(X_train['formula'].unique())\n",
    "print(f'But there are only {num_unique_formulae} unique formulae!\\n')\n",
    "\n",
    "print('Unique formulae and their number of occurances in the X_train DataFrame:')\n",
    "print(X_train['formula'].value_counts(), '\\n')\n",
    "print('Unique formulae and their number of occurances in the X_test DataFrame:')\n",
    "print(X_test['formula'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in total 3651 rows in the X_train DataFrame. But there are only 244 unique formulae!\n",
    "In fact, you will see that the same formulae are often present in the X_train and X_test DataFrames!\n",
    "\n",
    "That's not good, because now we have instances of the same chemical compound appearing in *both* the training and test data. Which means the model can cheat and in essence just memorize the training data, and during testing, look up the nearby values present in the training data!\n",
    "\n",
    "So how do we mitigate this?\n",
    "\n",
    "### Be aware of leaking data between datasets\n",
    "\n",
    "We have to first group the data by chemical formula, then split the data according to the chemical formulae. That way, all data points associated with each formula are either in the training dataset or in the test dataset, *but not in both at the same time*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data, cautiously (manually)\n",
    "\n",
    "First we get a list of all of the unique formulae in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_formulae = X['formula'].unique()\n",
    "print(f'{len(unique_formulae)} unique formulae:\\n{unique_formulae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed to ensure reproducibility across runs\n",
    "np.random.seed(seed=RNG_SEED)\n",
    "\n",
    "# Store a list of all unique formulae\n",
    "all_formulae = unique_formulae.copy()\n",
    "\n",
    "# Define the proportional size of the dataset split\n",
    "val_size = 0.20\n",
    "test_size = 0.10\n",
    "train_size = 1 - val_size - test_size\n",
    "\n",
    "# Calculate the number of samples in each dataset split\n",
    "num_val_samples = int(round(val_size * len(unique_formulae)))\n",
    "num_test_samples = int(round(test_size * len(unique_formulae)))\n",
    "num_train_samples = int(round((1 - val_size - test_size) * len(unique_formulae)))\n",
    "\n",
    "# Randomly choose the formulate for the validation dataset, and remove those from the unique formulae list\n",
    "val_formulae = np.random.choice(all_formulae, size=num_val_samples, replace=False)\n",
    "all_formulae = [f for f in all_formulae if f not in val_formulae]\n",
    "\n",
    "# Randomly choose the formulate for the test dataset, and remove those from the unique formulae list\n",
    "test_formulae = np.random.choice(all_formulae, size=num_test_samples, replace=False)\n",
    "all_formulae = [f for f in all_formulae if f not in test_formulae]\n",
    "\n",
    "# The remaining formulae will be used for the training dataset\n",
    "train_formulae = all_formulae.copy()\n",
    "\n",
    "print('Number of training formulae:', len(train_formulae))\n",
    "print('Number of validation formulae:', len(val_formulae))\n",
    "print('Number of testing formulae:', len(test_formulae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the original dataset into the train/validation/test datasets using the formulae lists above\n",
    "df_train = df[df['formula'].isin(train_formulae)]\n",
    "df_val = df[df['formula'].isin(val_formulae)]\n",
    "df_test = df[df['formula'].isin(test_formulae)]\n",
    "\n",
    "print(f'train dataset shape: {df_train.shape}')\n",
    "print(f'validation dataset shape: {df_val.shape}')\n",
    "print(f'test dataset shape: {df_test.shape}\\n')\n",
    "\n",
    "print(df_train.head(), '\\n')\n",
    "print(df_val.head(), '\\n')\n",
    "print(df_test.head(), '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure that we really only have mutually exclusive formulae within each of the datasets (e.g., all the data samples associated with \"Al2O3\" is *either* in the train, validation, or test dataset, but *not in multiple*), we can do the following to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formulae = set(df_train['formula'].unique())\n",
    "val_formulae = set(df_val['formula'].unique())\n",
    "test_formulae = set(df_test['formula'].unique())\n",
    "\n",
    "common_formulae1 = train_formulae.intersection(test_formulae)\n",
    "common_formulae2 = train_formulae.intersection(val_formulae)\n",
    "common_formulae3 = test_formulae.intersection(val_formulae)\n",
    "\n",
    "print(f'# of common formulae in intersection 1: {len(common_formulae1)}; common formulae: {common_formulae1}')\n",
    "print(f'# of common formulae in intersection 2: {len(common_formulae2)}; common formulae: {common_formulae2}')\n",
    "print(f'# of common formulae in intersection 3: {len(common_formulae3)}; common formulae: {common_formulae3}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save split datasets to csv\n",
    "\n",
    "Finally, after splitting the dataset into train/validation/test dataset splits, you can save them to disk for you to use later.\n",
    "\n",
    "By saving these dataset splits into files, you can then later reproducibly use these same exact splits during your subsequent model training and comparison steps.\n",
    "Use the same datasets for all your models---that way, you can ensure a fair comparison.\n",
    "\n",
    "Also, when you publish your results, you can include these dataset splits, so that others can use the exact datasets in their own studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving these splits into csv files\n",
    "PATH = os.getcwd()\n",
    "\n",
    "train_path = os.path.join(PATH, 'data/cp_train.csv')\n",
    "val_path = os.path.join(PATH, 'data/cp_val.csv')\n",
    "test_path = os.path.join(PATH, 'data/cp_test.csv')\n",
    "\n",
    "df_train.to_csv(train_path, index=False)\n",
    "df_val.to_csv(val_path, index=False)\n",
    "df_test.to_csv(test_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, keep the test dataset locked away and forget about it until you have finalized your model!\n",
    "**Never look at the test dataset!!** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "89d7d07a18bc6bcfb9fd99f9d58ef662eeaa9759129ecdb773c42f07a9c126b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
