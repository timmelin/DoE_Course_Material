{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NjparcaYib4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Teoroo-CMC/DoE_Course_Material/blob/main/MachineLearning/Jupyter-notebooks/merged.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "LVXVKDpnheST"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMMbnLNKgtOB"
      },
      "source": [
        "# Overview of Notebooks\n",
        "\n",
        "These notebooks are included to illustrate a hypothetical Machine Learning project created following best practices.\n",
        "\n",
        "The goal of this ML project is to predict the heat capacity of inorganic materials given the chemical composition and condition (the measurement temperature).\n",
        "We will use both classical ML models as well as neural networks.\n",
        "\n",
        "To do this, we must:\n",
        "1. Clean and process our dataset, removing obviously erroneous or empty values.\n",
        "1. Partition our data into train, validation, and test splits.\n",
        "1. Featurize our data, turning the chemical formulae into CBFVs.\n",
        "1. Train models on our data and assess the predictive power of the models.\n",
        "1. Compare the performance of the models fairly and reproducibly.\n",
        "1. Visualize the prediction results of the models.\n",
        "1. Share our models and enable others to reproduce your work and aid collaboration.\n",
        "\n",
        "\n",
        "If you require more information about how to use Jupyter notebooks, you can consult:\n",
        "* The main README file inside this repository: https://github.com/anthony-wang/BestPractices/blob/master/README.md\n",
        "* The official Jupyter Notebook documentation: https://jupyter-notebook.readthedocs.io/en/stable/notebook.html\n",
        "\n",
        "\n",
        "To read the main publication for which these notebooks are made, please see:\n",
        "\n",
        "Wang, Anthony Yu-Tung; Murdock, Ryan J.; Kauwe, Steven K.; Oliynyk, Anton O.; Gurlo, Aleksander; Brgoch, Jakoah; Persson, Kristin A.; Sparks, Taylor D., [Machine Learning for Materials Scientists: An Introductory Guide toward Best Practices](https://doi.org/10.1021/acs.chemmater.0c01907), *Chemistry of Materials* **Just Accepted Manuscript**, 2020. DOI: [10.1021/acs.chemmater.0c01907](https://doi.org/10.1021/acs.chemmater.0c01907)\n",
        "\n",
        "Please also consider citing the work if you choose to adopt or adapt the methods and concepts shown in these notebooks or in the publication:\n",
        "\n",
        "```bibtex\n",
        "@article{Wang2020bestpractices,\n",
        "    author = {Wang, Anthony Yu-Tung and Murdock, Ryan J. and Kauwe, Steven K. and Oliynyk, Anton O. and Gurlo, Aleksander and Brgoch, Jakoah and Persson, Kristin A. and Sparks, Taylor D.},\n",
        "    date = {2020},\n",
        "    title = {Machine Learning for Materials Scientists: An Introductory Guide toward Best Practices},\n",
        "    issn = {0897-4756},\n",
        "    journal = {Chemistry of Materials},\n",
        "    url = {https://doi.org/10.1021/acs.chemmater.0c01907},\n",
        "    doi = {10.1021/acs.chemmater.0c01907}\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7auMa0ggtOD"
      },
      "source": [
        "# Check that libraries are installed\n",
        "\n",
        "This notebook checks to see if you have the correct version of Python as well as all necessary libraries installed.\n",
        "\n",
        "Check the [main README file](https://github.com/anthony-wang/BestPractices/blob/master/README.md) for instructions if anything is missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QNDFtgNgtOE"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from distutils.version import LooseVersion as Version\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import curses\n",
        "    curses.setupterm()\n",
        "    assert curses.tigetnum(\"colors\") > 2\n",
        "    OK = \"\\x1b[1;%dm[ OK ]\\x1b[0m\" % (30 + curses.COLOR_GREEN)\n",
        "    FAIL = \"\\x1b[1;%dm[FAIL]\\x1b[0m\" % (30 + curses.COLOR_RED)\n",
        "except:\n",
        "    OK = '[ OK ]'\n",
        "    FAIL = '[FAIL]'\n",
        "\n",
        "try:\n",
        "    import importlib\n",
        "except ImportError:\n",
        "    print(FAIL, \"Python version 3.4 is required,\"\n",
        "                \" but %s is installed.\" % sys.version)\n",
        "\n",
        "def import_version(pkg, min_ver):\n",
        "    mod = None\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        if pkg in {'PIL'}:\n",
        "            ver = mod.VERSION\n",
        "        else:\n",
        "            ver = mod.__version__\n",
        "        if Version(ver) < min_ver:\n",
        "            print(FAIL, \"%s version %s or higher required, but %s installed.\"\n",
        "                  % (lib, min_ver, ver))\n",
        "        else:\n",
        "            print(OK, '%s version %s' % (pkg, ver))\n",
        "    except ImportError as imp_err_msg:\n",
        "        print(FAIL, 'Error in importing %s: %s' % (pkg, imp_err_msg))\n",
        "    except AttributeError as att_err_msg:\n",
        "        print(FAIL, 'Error in reading attribute of %s: %s' % (pkg, att_err_msg))\n",
        "    return mod\n",
        "\n",
        "# first check the python version\n",
        "print('Using python in', sys.prefix)\n",
        "print(sys.version)\n",
        "pyversion = Version(sys.version)\n",
        "if pyversion >= \"3\":\n",
        "    if pyversion < \"3.7\":\n",
        "        print(FAIL, \"Python version > 3.7 is required,\"\n",
        "                    \" but %s is installed.\\n\" % sys.version)\n",
        "elif pyversion < \"3\":\n",
        "    print(FAIL, \"Python version > 3.7 is required,\"\n",
        "                \" but %s is installed.\\n\" % sys.version)\n",
        "else:\n",
        "    print(FAIL, \"Unknown Python version: %s\\n\" % sys.version)\n",
        "\n",
        "requirements = {'numpy': '1.18.0',\n",
        "                'pandas': '1.0.0',\n",
        "                'pandas_profiling': '2.4.0',\n",
        "                'matplotlib': '3.2.0',\n",
        "                'seaborn': '0.10.0',\n",
        "                'sklearn': '0.22.0',\n",
        "                'scipy': '1.4.0',\n",
        "                'tqdm': '4.43.0',\n",
        "                'jupyter_client': '6.0.0',\n",
        "                'ipywidgets': '7.5.0',\n",
        "                'torch': '1.3.0',}\n",
        "\n",
        "# now check the dependencies\n",
        "for lib, required_version in list(requirements.items()):\n",
        "    import_version(lib, required_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNCHcOlrgtOF"
      },
      "source": [
        "# Data loading, cleanup and processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc8ldpVggtOG"
      },
      "source": [
        "The first step to a ML project is to obtain the dataset you will be working with.\n",
        "There are many repositories for materials science-specific data (whether online or offline)---consult the accompanying paper for a list of the more commonly used ones.\n",
        "\n",
        "Once you have identified the repository and dataset you will use for your project, you will have to download it to your local machine, or establish a way to reliably access the dataset.\n",
        "Consult the documentation of the repository for how to do this.\n",
        "\n",
        "For this tutorial, we have collected heat capacity ($C_p$) data from the [NIST-JANAF Thermochemical Tables](https://doi.org/10.18434/T42S31)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSFUgNYbgtOI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "from ydata_profiling import ProfileReport"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cJu8VDogtOI"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Using Pandas, we read in the dataset into a DataFrame.\n",
        "\n",
        "We also print the shape of the DataFrame, which indicates the number of rows and columns in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS_AgLx8gtOJ"
      },
      "outputs": [],
      "source": [
        "PATH = os.getcwd()\n",
        "data_path = os.path.join(PATH, 'data/cp_data_demo.csv')\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "print(f'Original DataFrame shape: {df.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCOOKDNvgtOJ"
      },
      "source": [
        "This means that our input dataset has 4583 data samples, each with 3 variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8wAhG8vgtOK"
      },
      "source": [
        "## Examine the data\n",
        "\n",
        "We examine some rows and look at the data's basic statistics.\n",
        "\n",
        "We see that the dataset contains information about the formula, measurement condition (in this case, temperature in K), and the target property, heat capacity (in J/(mol * K))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxyP1_AcgtOL"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zt2zQzGgtOL"
      },
      "source": [
        "First thing you should notice: we have many observations of the same compound (B2O3) but measured at different measurement conditions, resulting in a different property value.\n",
        "\n",
        "We can get some simple summary statistics of the DataFrame by calling the `.describe()` method on the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7h2oIe_gtOL"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huwjh6JegtOM"
      },
      "source": [
        "Using the `pandas-profiling` library, we can generate a more in-depth report of our starting dataset.\n",
        "Note that generating this profile report might take upwards of 20 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAH1to4CgtOM"
      },
      "outputs": [],
      "source": [
        "profile = ProfileReport(df.copy(), title='Pandas Profiling Report of Cp dataset', html={'style':{'full_width':True}})\n",
        "profile.to_widgets()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwGL0JBbgtON"
      },
      "source": [
        "Notice a few things from the profile report:\n",
        "* We have some missing cells in the dataset (\"Overview\" tab)\n",
        "* We have some unrealistic Temperature and Heat Capacity values in the dataset (\"Variables\" tab)\n",
        "* We have some missing Temperature, Formula and Heat Capacity values in the dataset (\"Variables\" tab)\n",
        "\n",
        "Also notice that on the \"Overview\" tab, there is the following warning: `FORMULA` has a high cardinality: 245 distinct values.\n",
        "\n",
        "Cardinality is the number of distinct values in a column of a table, relative to the number of rows in the table.\n",
        "In our dataset, we have a total of 4583 data observations, but only 245 distinct formulae.\n",
        "We will have to keep this in mind later, when we process and split the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrlaxTH7gtON"
      },
      "source": [
        "## Rename the column names for brevity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXku9gQggtON"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJbHijEggtOO"
      },
      "outputs": [],
      "source": [
        "rename_dict = {'FORMULA': 'formula',\n",
        "               'CONDITION: Temperature (K)': 'T',\n",
        "               'PROPERTY: Heat Capacity (J/mol K)': 'Cp'}\n",
        "df = df.rename(columns=rename_dict)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCwFYaD3gtOO"
      },
      "source": [
        "## Check for and remove `NaN` values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMuln4OqgtOO"
      },
      "source": [
        "Here we can use the built-in Pandas methods to check for `NaN` values in the dataset, which are missing values.\n",
        "We then remove the dataset rows which contain `NaN` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdScTPNdgtOP"
      },
      "outputs": [],
      "source": [
        "# Check for NaNs in the respective dataset columns, and get the indices\n",
        "df2 = df.copy()\n",
        "bool_nans_formula = df2['formula'].isnull()\n",
        "bool_nans_T = df2['T'].isnull()\n",
        "bool_nans_Cp = df2['Cp'].isnull()\n",
        "\n",
        "# Drop the rows of the DataFrame which contain NaNs\n",
        "df2 = df2.drop(df2.loc[bool_nans_formula].index, axis=0)\n",
        "df2 = df2.drop(df2.loc[bool_nans_T].index, axis=0)\n",
        "df2 = df2.drop(df2.loc[bool_nans_Cp].index, axis=0)\n",
        "\n",
        "print(f'DataFrame shape before dropping NaNs: {df.shape}')\n",
        "print(f'DataFrame shape after dropping NaNs: {df2.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-YdZKeDgtOP"
      },
      "source": [
        "Pandas also includes the convenient built-in method `.dropna()` to check for and remove `NaNs` in-place:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1gwyUmCgtOP"
      },
      "outputs": [],
      "source": [
        "df3 = df.copy()\n",
        "df3 = df3.dropna(axis=0, how='any')\n",
        "\n",
        "print(f'DataFrame shape before dropping NaNs: {df.shape}')\n",
        "print(f'DataFrame shape after dropping NaNs: {df3.shape}')\n",
        "\n",
        "df = df3.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpXdFPovgtOQ"
      },
      "source": [
        "## Check for and remove unrealistic values\n",
        "\n",
        "In some cases, you might also get data values that simply don't make sense.\n",
        "For our dase, this could be negative values in the temperature or heat capacity values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tVQe186gtOQ"
      },
      "outputs": [],
      "source": [
        "bool_invalid_T = df['T'] < 0\n",
        "bool_invalid_Cp = df['Cp'] < 0\n",
        "\n",
        "df = df.drop(df.loc[bool_invalid_T].index, axis=0)\n",
        "df = df.drop(df.loc[bool_invalid_Cp].index, axis=0)\n",
        "\n",
        "print(f'Cleaned DataFrame shape: {df.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsK14U37gtOQ"
      },
      "source": [
        "## Save cleaned data to csv\n",
        "\n",
        "Finally, after cleaning and processing the data, you can save it to disk in a cleaned state for you to use later.\n",
        "\n",
        "Pandas allows us to save our data as a comma separated value `.csv` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1orDVDDugtOQ"
      },
      "outputs": [],
      "source": [
        "out_path = os.path.join(PATH, 'data/cp_data_cleaned.csv')\n",
        "df.to_csv(out_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkKGBFZigtOR"
      },
      "source": [
        "Note, your data can be saved in other file formats (such as hdf5) or in databases (such as SQL), but we will not go into the details of these formats.\n",
        "\n",
        "Typically, the amount of data you can gather for your ML project isn't large enough to warrant these approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPrkoioJgtOR"
      },
      "source": [
        "# Splitting data into the train/validation/test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVdb_4o7gtOR"
      },
      "source": [
        "It is important to split your full dataset into train/validation/test datasets, and reliably use the same datasets for your modeling tasks later.\n",
        "\n",
        "Using different train/validation/test splits can dramatically affect your model performance (as seen here by the variance in $r^2$ scores for 30 models which have been trained on 30 different dataset splits) [1]:\n",
        "\n",
        "<div>\n",
        "<img src=\"./images/Fig1_30_splits.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "[1]: C. Clement, S. K. Kauwe, T. D. Sparks, Benchmark AFLOW Data Sets for Machine Learning, figshare 2020, DOI: [10.6084/m9.figshare.11954742](https://dx.doi.org/10.6084/m9.figshare.11954742)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP--HM4ggtOR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set a random seed to ensure reproducibility across runs\n",
        "RNG_SEED = 42\n",
        "np.random.seed(seed=RNG_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfMOJY_igtOR"
      },
      "source": [
        "## Load the pre-processed dataset\n",
        "\n",
        "We will start with the processed dataset that we saved from the last notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWM1d3aAgtOS"
      },
      "outputs": [],
      "source": [
        "PATH = os.getcwd()\n",
        "data_path = os.path.join(PATH, 'data/cp_data_cleaned.csv')\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "print(f'Full DataFrame shape: {df.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRaqKORPgtOS"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ULboshhgtOS"
      },
      "source": [
        "## Separate the DataFrame into your input variables ($X$) and target variables ($y$)\n",
        "\n",
        "The $X$ will be used as the input data, and $y$ will be used as the prediction targets for your ML model.\n",
        "\n",
        "If your target variables are discrete (such as `metal`/`non-metal` or types of crystal structures), then you will be performing a classification task.\n",
        "In our case, since our target variables are continuous values (heat capacity), we are performing a regression task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vz3uM06gtOS"
      },
      "outputs": [],
      "source": [
        "X = df[['formula', 'T']]\n",
        "y = df['Cp']\n",
        "\n",
        "print(f'Shape of X: {X.shape}')\n",
        "print(f'Shape of y: {y.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbrCiSpMgtOT"
      },
      "source": [
        "## Splitting data (and a word of caution)\n",
        "### Normally, we could simply split the data with a simple `sklearn` function\n",
        "\n",
        "The scikit-learn `train_test_split` function randomly splits a dataset into train and test datasets.\n",
        "Typically, you can use `train_test_split` to first split your data into \"train\" and \"test\" datasets, and then use the function again to split your \"train\" data into \"train\" and \"validation\" dataset splits.\n",
        "\n",
        "As a rule of thumb, you can roughly aim for the following dataset proportions when splitting your data:\n",
        "\n",
        "| | train split | validation split | test split |\n",
        "| --- | --- | --- | --- |\n",
        "| proportion<br> of original<br> dataset | 50% to 70% | 20% to 30% | 10% to 20% |\n",
        "\n",
        "If you have copious amounts of data, it may suffice to train your models on just 50% of the data; that way, you have a larger amount of data samples to validate and to test with.\n",
        "If you however have a smaller dataset and thus very few training samples for your models, you may wish to increase your proportion of training data during dataset splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_75J1TMKgtOT"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RNG_SEED)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w-MlLZ9gtOT"
      },
      "source": [
        "### But wait, what's wrong here?\n",
        "\n",
        "We have to make sure that our dataset splits contain mutually exclusive formulae (e.g., all the data samples associated with \"Al2O3\" is *either* in the train, validation, or test dataset, but *not in multiple*)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97Qc2LJGgtOT"
      },
      "outputs": [],
      "source": [
        "num_rows = len(X_train)\n",
        "print(f'There are in total {num_rows} rows in the X_train DataFrame.')\n",
        "\n",
        "num_unique_formulae = len(X_train['formula'].unique())\n",
        "print(f'But there are only {num_unique_formulae} unique formulae!\\n')\n",
        "\n",
        "print('Unique formulae and their number of occurances in the X_train DataFrame:')\n",
        "print(X_train['formula'].value_counts(), '\\n')\n",
        "print('Unique formulae and their number of occurances in the X_test DataFrame:')\n",
        "print(X_test['formula'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F8ld5wlgtOU"
      },
      "source": [
        "There are in total 3651 rows in the X_train DataFrame. But there are only 244 unique formulae!\n",
        "In fact, you will see that the same formulae are often present in the X_train and X_test DataFrames!\n",
        "\n",
        "That's not good, because now we have instances of the same chemical compound appearing in *both* the training and test data. Which means the model can cheat and in essence just memorize the training data, and during testing, look up the nearby values present in the training data!\n",
        "\n",
        "So how do we mitigate this?\n",
        "\n",
        "### Be aware of leaking data between datasets\n",
        "\n",
        "We have to first group the data by chemical formula, then split the data according to the chemical formulae. That way, all data points associated with each formula are either in the training dataset or in the test dataset, *but not in both at the same time*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPH5Vu9VgtOU"
      },
      "source": [
        "## Splitting data, cautiously (manually)\n",
        "\n",
        "First we get a list of all of the unique formulae in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EqBvrrHgtOU"
      },
      "outputs": [],
      "source": [
        "unique_formulae = X['formula'].unique()\n",
        "print(f'{len(unique_formulae)} unique formulae:\\n{unique_formulae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXFJpWTkgtOU"
      },
      "outputs": [],
      "source": [
        "# Set a random seed to ensure reproducibility across runs\n",
        "np.random.seed(seed=RNG_SEED)\n",
        "\n",
        "# Store a list of all unique formulae\n",
        "all_formulae = unique_formulae.copy()\n",
        "\n",
        "# Define the proportional size of the dataset split\n",
        "val_size = 0.20\n",
        "test_size = 0.10\n",
        "train_size = 1 - val_size - test_size\n",
        "\n",
        "# Calculate the number of samples in each dataset split\n",
        "num_val_samples = int(round(val_size * len(unique_formulae)))\n",
        "num_test_samples = int(round(test_size * len(unique_formulae)))\n",
        "num_train_samples = int(round((1 - val_size - test_size) * len(unique_formulae)))\n",
        "\n",
        "# Randomly choose the formulate for the validation dataset, and remove those from the unique formulae list\n",
        "val_formulae = np.random.choice(all_formulae, size=num_val_samples, replace=False)\n",
        "all_formulae = [f for f in all_formulae if f not in val_formulae]\n",
        "\n",
        "# Randomly choose the formulate for the test dataset, and remove those from the unique formulae list\n",
        "test_formulae = np.random.choice(all_formulae, size=num_test_samples, replace=False)\n",
        "all_formulae = [f for f in all_formulae if f not in test_formulae]\n",
        "\n",
        "# The remaining formulae will be used for the training dataset\n",
        "train_formulae = all_formulae.copy()\n",
        "\n",
        "print('Number of training formulae:', len(train_formulae))\n",
        "print('Number of validation formulae:', len(val_formulae))\n",
        "print('Number of testing formulae:', len(test_formulae))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5tu1G5GgtOV"
      },
      "outputs": [],
      "source": [
        "# Split the original dataset into the train/validation/test datasets using the formulae lists above\n",
        "df_train = df[df['formula'].isin(train_formulae)]\n",
        "df_val = df[df['formula'].isin(val_formulae)]\n",
        "df_test = df[df['formula'].isin(test_formulae)]\n",
        "\n",
        "print(f'train dataset shape: {df_train.shape}')\n",
        "print(f'validation dataset shape: {df_val.shape}')\n",
        "print(f'test dataset shape: {df_test.shape}\\n')\n",
        "\n",
        "print(df_train.head(), '\\n')\n",
        "print(df_val.head(), '\\n')\n",
        "print(df_test.head(), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTCIp24wgtOV"
      },
      "source": [
        "To be sure that we really only have mutually exclusive formulae within each of the datasets (e.g., all the data samples associated with \"Al2O3\" is *either* in the train, validation, or test dataset, but *not in multiple*), we can do the following to check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJsS20nCgtOV"
      },
      "outputs": [],
      "source": [
        "train_formulae = set(df_train['formula'].unique())\n",
        "val_formulae = set(df_val['formula'].unique())\n",
        "test_formulae = set(df_test['formula'].unique())\n",
        "\n",
        "common_formulae1 = train_formulae.intersection(test_formulae)\n",
        "common_formulae2 = train_formulae.intersection(val_formulae)\n",
        "common_formulae3 = test_formulae.intersection(val_formulae)\n",
        "\n",
        "print(f'# of common formulae in intersection 1: {len(common_formulae1)}; common formulae: {common_formulae1}')\n",
        "print(f'# of common formulae in intersection 2: {len(common_formulae2)}; common formulae: {common_formulae2}')\n",
        "print(f'# of common formulae in intersection 3: {len(common_formulae3)}; common formulae: {common_formulae3}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ_ps0BHgtOW"
      },
      "source": [
        "## Save split datasets to csv\n",
        "\n",
        "Finally, after splitting the dataset into train/validation/test dataset splits, you can save them to disk for you to use later.\n",
        "\n",
        "By saving these dataset splits into files, you can then later reproducibly use these same exact splits during your subsequent model training and comparison steps.\n",
        "Use the same datasets for all your models---that way, you can ensure a fair comparison.\n",
        "\n",
        "Also, when you publish your results, you can include these dataset splits, so that others can use the exact datasets in their own studies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4-aOn-HgtOW"
      },
      "outputs": [],
      "source": [
        "# saving these splits into csv files\n",
        "PATH = os.getcwd()\n",
        "\n",
        "train_path = os.path.join(PATH, 'data/cp_train.csv')\n",
        "val_path = os.path.join(PATH, 'data/cp_val.csv')\n",
        "test_path = os.path.join(PATH, 'data/cp_test.csv')\n",
        "\n",
        "df_train.to_csv(train_path, index=False)\n",
        "df_val.to_csv(val_path, index=False)\n",
        "df_test.to_csv(test_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc_KgNltgtOW"
      },
      "source": [
        "Remember, keep the test dataset locked away and forget about it until you have finalized your model!\n",
        "**Never look at the test dataset!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtcfibdYgtOY"
      },
      "source": [
        "# Data Featurization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-8AY3k5gtOY"
      },
      "source": [
        "Here, we will show some simple examples of featurizing materials composition data using so-called \"composition-based feature vectors\", or CBFVs. This methods represents a single chemical formula as one vector based on its constituent atoms' chemical properties (refer to the paper for more information and references).\n",
        "\n",
        "Note that the steps shown in this notebook are intended to demonstrate the best practices associated with featurizing materials data, using *one* way of featurizing materials composition data as an example.\n",
        "Depending on your input data and your particular modeling needs, the data featurization method and procedure you use may be different than the example shown here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yltBmtVgtOY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Set a random seed to ensure reproducibility across runs\n",
        "RNG_SEED = 42\n",
        "np.random.seed(RNG_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj9JekaigtOY"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "We will start with the dataset splits that we saved from the last notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCK1B1ftgtOZ"
      },
      "outputs": [],
      "source": [
        "PATH = os.getcwd()\n",
        "train_path = os.path.join(PATH, 'data/cp_train.csv')\n",
        "val_path = os.path.join(PATH, 'data/cp_val.csv')\n",
        "test_path = os.path.join(PATH, 'data/cp_test.csv')\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_val = pd.read_csv(val_path)\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "print(f'df_train DataFrame shape: {df_train.shape}')\n",
        "print(f'df_val DataFrame shape: {df_val.shape}')\n",
        "print(f'df_test DataFrame shape: {df_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2psIri59gtOZ"
      },
      "source": [
        "## Sub-sampling your data (optional)\n",
        "\n",
        "If your dataset is too large, you can subsample it to be a smaller size.\n",
        "This is useful for prototyping and for making quick sanity tests of new models / parameters.\n",
        "\n",
        "Just be aware that you do not introduce any bias into your data through the sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHfWMNRygtOZ"
      },
      "outputs": [],
      "source": [
        "# Sub-sample the data. Set the random_state to make the sampling reproducible every time.\n",
        "df_train_sampled = df_train.sample(n=2000, random_state=RNG_SEED)\n",
        "df_val_sampled = df_val.sample(n=200, random_state=RNG_SEED)\n",
        "df_test_sampled = df_test.sample(n=200, random_state=RNG_SEED)\n",
        "\n",
        "print(f'df_train_sampled DataFrame shape: {df_train_sampled.shape}')\n",
        "print(f'df_val_sampled DataFrame shape: {df_val_sampled.shape}')\n",
        "print(f'df_test_sampled DataFrame shape: {df_test_sampled.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT9qWKVSgtOa"
      },
      "source": [
        "## Generate features using the `CBFV` package\n",
        "\n",
        "To featurize the chemical compositions from a chemical formula (e.g. \"Al2O3\") into a composition-based feature vector (CBFV), we use the open-source [`CBFV` package](https://github.com/kaaiian/CBFV).\n",
        "\n",
        "We have downloaded and saved a local copy of the package into this repository for your convenience.\n",
        "For the most updated version, refer to the GitHub repository linked above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UXzxKGZgtOa"
      },
      "outputs": [],
      "source": [
        "# Import the package and the generate_features function\n",
        "from CBFV.cbfv.composition import generate_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW8H-UIngtOa"
      },
      "source": [
        "The `generate_features` function from the CBFV package expects an input DataFrame containing at least the columns `['formula', 'target']`. You may also have extra feature columns (e.g., `temperature` or `pressure`, other measurement conditions, etc.).\n",
        "\n",
        "In our dataset, `Cp` represents the target variable, and `T` is the measurement condition.\n",
        "Since the `generate_features` function expects the target variable column to be named `target`, we have to rename the `Cp` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-tVYHtDgtOa"
      },
      "outputs": [],
      "source": [
        "print('DataFrame column names before renaming:')\n",
        "print(df_train.columns)\n",
        "print(df_val.columns)\n",
        "print(df_test.columns)\n",
        "\n",
        "rename_dict = {'Cp': 'target'}\n",
        "df_train = df_train.rename(columns=rename_dict)\n",
        "df_val = df_val.rename(columns=rename_dict)\n",
        "df_test = df_test.rename(columns=rename_dict)\n",
        "\n",
        "df_train_sampled = df_train_sampled.rename(columns=rename_dict)\n",
        "df_val_sampled = df_val_sampled.rename(columns=rename_dict)\n",
        "df_test_sampled = df_test_sampled.rename(columns=rename_dict)\n",
        "\n",
        "print('\\nDataFrame column names after renaming:')\n",
        "print(df_train.columns)\n",
        "print(df_val.columns)\n",
        "print(df_test.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW1bvwuvgtOa"
      },
      "source": [
        "Now we can use the `generate_features` function to generate the CBFVs from the input data.\n",
        "\n",
        "Note that we have specified several keyword arguments in our call to `generate_features`:\n",
        "* `elem_prop='oliynyk'`\n",
        "* `drop_duplicates=False`\n",
        "* `extend_features=True`\n",
        "* `sum_feat=True`\n",
        "\n",
        "A short explanation for the choice of keyword arguments is below:\n",
        "* The `elem_prop` parameter specifies which CBFV featurization scheme to use (there are several). For this tutorial, we have chosen to use the `oliynyk` CBFV featurization scheme.\n",
        "* The `drop_duplicates` parameter specifies whether to drop duplicate formulae during featurization. In our case, we want to preserve duplicate formulae in our data (`True`), since we have multiple heat capacity measurements (performed at different temperatures) for the same compound.\n",
        "* The `extend_features` parameter specifies whether to include extended features (features that are not part of `['formula', 'target']`) in the featurized data. In our case, this is our measurement temperature, and we want to include this information (`True`), since this is pertinent information for the heat capacity prediction.\n",
        "* The `sum_feat` parameter specifies whether to calculate the sum features when generating the CBFVs for the chemical formulae. We do in our case (`True`).\n",
        "\n",
        "For more information about the `generate_features` function and the CBFV featurization scheme, refer to the GitHub repository and the accompanying paper to this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex-OSt8XgtOb"
      },
      "outputs": [],
      "source": [
        "X_train_unscaled, y_train, formulae_train, skipped_train = generate_features(df_train_sampled, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
        "X_val_unscaled, y_val, formulae_val, skipped_val = generate_features(df_val_sampled, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
        "X_test_unscaled, y_test, formulae_test, skipped_test = generate_features(df_test_sampled, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPFBvemngtOb"
      },
      "source": [
        "To see what a featurized X matrix looks like, `.head()` will show us some rows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD62d5TggtOb"
      },
      "outputs": [],
      "source": [
        "X_train_unscaled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CySZy4O0gtOb"
      },
      "outputs": [],
      "source": [
        "X_train_unscaled.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk9kmf3FgtOb"
      },
      "source": [
        "Note the `sum` features in the CBFV, which we have included by using `sum_feat=True` in the call to `generate_features`.\n",
        "\n",
        "Also note the temperature column `T` at the end of this featurized data.\n",
        "\n",
        "What we have done above is featurize the input data. In the featurized data, each row contains a unique CBFV that describes a given chemical composition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCjcQjAcgtOc"
      },
      "source": [
        "## Data scaling & normalization\n",
        "\n",
        "For numerical input data, scaling and normalization of the features often improves the model performance.\n",
        "Scaling can partially correct the discrepancy between the orders of magnitudes of the features (e.g., some numerical features being much larger or smaller than others).\n",
        "This typically improves the model learning performance, and in turn, improves the model performance.\n",
        "\n",
        "We will scale then normalize our input data using scikit-learn's built-in `StandardScaler` class and `normalize` function.\n",
        "\n",
        "Note, in addition to `StandardScaler`, other scalers such as `RobustScaler` and `MinMaxScaler` are also available in scikit-learn. Consult the documentation for the details and when to use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--gcBqlRgtOc"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGtdtMrqgtOc"
      },
      "source": [
        "## Scaling the data\n",
        "\n",
        "First, we instantiate the scaler object.\n",
        "\n",
        "In a `StandardScaler` object:\n",
        "* During the `fit` process, the statistics of the input data (mean and standard deviation) are computed.\n",
        "* Then, during the `transform` process, the mean and standard deviation values calculated above are used to scale the data to having zero-mean and unit variance.\n",
        "\n",
        "Therefore, for the first time usage of the scaler, we call the `.fit_transform()` method to fit the scaler to the input data, and then to transform the same data.\n",
        "For subsequent uses, since we have already computed the statistics, we only call the `.transform()` method to scale data.\n",
        "\n",
        "**Note:** you should *only* `.fit()` the scaler using the training dataset statistics, and then use these same statistics from the training dataset to `.transform()` the other datasets (validation and train)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSFPa4uigtOc"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train_unscaled)\n",
        "X_val = scaler.transform(X_val_unscaled)\n",
        "X_test = scaler.transform(X_test_unscaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDVqGdOxgtOd"
      },
      "source": [
        "## Normalizing the scaled data\n",
        "\n",
        "We repeat a similar process for normalizing the data.\n",
        "Here, there is no need to first fit the normalizer, since the normalizer scales the rows of the input data to unit norm independently of other rows.\n",
        "\n",
        "The normalizer is different to a Scaler in that the normalizer acts row-wise, whereas a Scaler acts column-wise on the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFsk6NQIgtOd"
      },
      "outputs": [],
      "source": [
        "X_train = normalize(X_train)\n",
        "X_val = normalize(X_val)\n",
        "X_test = normalize(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1ZIhoRHgtOd"
      },
      "source": [
        "# Modeling using \"classical\" machine learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0908WI7gtOd"
      },
      "source": [
        "Here we implement some classical ML models from `sklearn`:\n",
        "\n",
        "* Ridge regression\n",
        "* Support vector machine\n",
        "* Linear support vector machine\n",
        "* Random forest\n",
        "* Extra trees\n",
        "* Adaptive boosting\n",
        "* Gradient boosting\n",
        "* k-nearest neighbors\n",
        "* Dummy (if you can't beat this, something is wrong.)\n",
        "\n",
        "Note: the Dummy model types from `sklearn` act as a good sanity check for your ML studies. If your models do not perform significantly better than the equivalent Dummy models, then you should know that something has gone wrong in your model implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4IVd6PDgtOe"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "from sklearn.dummy import DummyRegressor\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA28Xn0bgtOe"
      },
      "source": [
        "In addition, we define some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LsyR4NugtOe"
      },
      "outputs": [],
      "source": [
        "def instantiate_model(model_name):\n",
        "    model = model_name()\n",
        "    return model\n",
        "\n",
        "def fit_model(model, X_train, y_train):\n",
        "    ti = time()\n",
        "    model = instantiate_model(model)\n",
        "    model.fit(X_train, y_train)\n",
        "    fit_time = time() - ti\n",
        "    return model, fit_time\n",
        "\n",
        "def evaluate_model(model, X, y_act):\n",
        "    y_pred = model.predict(X)\n",
        "    r2 = r2_score(y_act, y_pred)\n",
        "    mae = mean_absolute_error(y_act, y_pred)\n",
        "    rmse_val = mean_squared_error(y_act, y_pred, squared=False)\n",
        "    return r2, mae, rmse_val\n",
        "\n",
        "def fit_evaluate_model(model, model_name, X_train, y_train, X_val, y_act_val):\n",
        "    model, fit_time = fit_model(model, X_train, y_train)\n",
        "    r2_train, mae_train, rmse_train = evaluate_model(model, X_train, y_train)\n",
        "    r2_val, mae_val, rmse_val = evaluate_model(model, X_val, y_act_val)\n",
        "    result_dict = {\n",
        "        'model_name': model_name,\n",
        "        'model_name_pretty': type(model).__name__,\n",
        "        'model_params': model.get_params(),\n",
        "        'fit_time': fit_time,\n",
        "        'r2_train': r2_train,\n",
        "        'mae_train': mae_train,\n",
        "        'rmse_train': rmse_train,\n",
        "        'r2_val': r2_val,\n",
        "        'mae_val': mae_val,\n",
        "        'rmse_val': rmse_val}\n",
        "    return model, result_dict\n",
        "\n",
        "def append_result_df(df, result_dict):\n",
        "    df_result_appended = df.append(result_dict, ignore_index=True)\n",
        "    return df_result_appended\n",
        "\n",
        "def append_model_dict(dic, model_name, model):\n",
        "    dic[model_name] = model\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyHxUst-gtOf"
      },
      "source": [
        "Build an empty DataFrame to store model results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GK9nlYggtOf"
      },
      "outputs": [],
      "source": [
        "df_classics = pd.DataFrame(columns=['model_name',\n",
        "                                    'model_name_pretty',\n",
        "                                    'model_params',\n",
        "                                    'fit_time',\n",
        "                                    'r2_train',\n",
        "                                    'mae_train',\n",
        "                                    'rmse_train',\n",
        "                                    'r2_val',\n",
        "                                    'mae_val',\n",
        "                                    'rmse_val'])\n",
        "df_classics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X72gGW2YgtOf"
      },
      "source": [
        "## Define the models\n",
        "\n",
        "Here, we instantiate several classical machine learning models for use.\n",
        "For demonstration purposes, we instantiate the models with their default model parameters.\n",
        "\n",
        "Some of the models listed above can perform either regression or classification tasks.\n",
        "Because our ML task is a regression task (prediction of the continuous-valued target, heat capacity), we choose the regression variant of these models.\n",
        "\n",
        "Note: the `DummyRegressor()` instance acts as a good sanity check for your ML studies. If your models do not perform significantly better than the `DummyRegressor()`, then you know something has gone awry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YefkvQjHgtOf"
      },
      "outputs": [],
      "source": [
        "# Build a dictionary of model names\n",
        "classic_model_names = OrderedDict({\n",
        "    'dumr': DummyRegressor,\n",
        "    'rr': Ridge,\n",
        "    'abr': AdaBoostRegressor,\n",
        "    'gbr': GradientBoostingRegressor,\n",
        "    'rfr': RandomForestRegressor,\n",
        "    'etr': ExtraTreesRegressor,\n",
        "    'svr': SVR,\n",
        "    'lsvr': LinearSVR,\n",
        "    'knr': KNeighborsRegressor,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4R7kOHFgtOg"
      },
      "source": [
        "## Instantiate and fit the models\n",
        "\n",
        "Now, we can fit the ML models.\n",
        "\n",
        "We will loop through each of the models listed above. For each of the models, we will:\n",
        "* instantiate the model (with default parameters)\n",
        "* fit the model using the training data\n",
        "* use the fitted model to generate predictions from the validation data\n",
        "* evaluate the performance of the model using the predictions\n",
        "* store the results in a DataFrame for analysis\n",
        "\n",
        "Note: this may take several minutes, depending on your hardware/software environment, dataset size and featurization scheme (CBFV)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq5TSijpgtOg"
      },
      "outputs": [],
      "source": [
        "# Instantiate a dictionary to store the model objects\n",
        "classic_models = OrderedDict()\n",
        "\n",
        "# Keep track of elapsed time\n",
        "ti = time()\n",
        "\n",
        "# Loop through each model type, fit and predict, and evaluate and store results\n",
        "for model_name, model in classic_model_names.items():\n",
        "    print(f'Now fitting and evaluating model {model_name}: {model.__name__}')\n",
        "    model, result_dict = fit_evaluate_model(model, model_name, X_train, y_train, X_val, y_val)\n",
        "    df_classics = append_result_df(df_classics, result_dict)\n",
        "    classic_models = append_model_dict(classic_models, model_name, model)\n",
        "\n",
        "dt = time() - ti\n",
        "print(f'Finished fitting {len(classic_models)} models, total time: {dt:0.2f} s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J84y9KKVgtOg"
      },
      "source": [
        "Now, we can look at the results.\n",
        "\n",
        "You will notice, that some of the models (such as RandomForestRegressor, ExtraTreesRegressor and GradientBoostingRegressor) have completely memorized the training data, as evidenced by the very high r2_train scores of ~1.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U03pQCdgtOg"
      },
      "outputs": [],
      "source": [
        "# Sort in order of increasing validation r2 score\n",
        "df_classics = df_classics.sort_values('r2_val', ignore_index=True)\n",
        "df_classics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgdBR0K0gtOh"
      },
      "source": [
        "You can now also access the full details of the models by inspecting the `classic_models` dictionary that we populated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0feSg7JgtOh"
      },
      "outputs": [],
      "source": [
        "classic_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g95vQtSBgtOh"
      },
      "source": [
        "## Evaluating model performance on validation dataset\n",
        "\n",
        "Now comes the time to evaluate the trained models on the validation set.\n",
        "\n",
        "Remember, we use the same validation set to evaluate all models. This ensures a fair comparison.\n",
        "\n",
        "In addition, we plot the predicted vs. actual plots using the predictions made by each trained model on the same validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2MNvQxsgtOh"
      },
      "outputs": [],
      "source": [
        "def plot_pred_act(act, pred, model, reg_line=True, label=''):\n",
        "    xy_max = np.max([np.max(act), np.max(pred)])\n",
        "\n",
        "    plot = plt.figure(figsize=(6,6))\n",
        "    plt.plot(act, pred, 'o', ms=9, mec='k', mfc='silver', alpha=0.4)\n",
        "    plt.plot([0, xy_max], [0, xy_max], 'k--', label='ideal')\n",
        "    if reg_line:\n",
        "        polyfit = np.polyfit(act, pred, deg=1)\n",
        "        reg_ys = np.poly1d(polyfit)(np.unique(act))\n",
        "        plt.plot(np.unique(act), reg_ys, alpha=0.8, label='linear fit')\n",
        "    plt.axis('scaled')\n",
        "    plt.xlabel(f'Actual {label}')\n",
        "    plt.ylabel(f'Predicted {label}')\n",
        "    plt.title(f'{type(model).__name__}, r2: {r2_score(act, pred):0.4f}')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    return plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLC4lbucgtOh"
      },
      "outputs": [],
      "source": [
        "for row in range(df_classics.shape[0]):\n",
        "    model_name = df_classics.iloc[row]['model_name']\n",
        "\n",
        "    model = classic_models[model_name]\n",
        "    y_act_val = y_val\n",
        "    y_pred_val = model.predict(X_val)\n",
        "\n",
        "    plot = plot_pred_act(y_act_val, y_pred_val, model, reg_line=True, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jZej0drgtOi"
      },
      "source": [
        "## Re-training the best-performing model on combined train + validation dataset\n",
        "\n",
        "After you have finalized your model, you can re-train your model (using the same hyperparameters) again on the combined train + validation datasets, and finally, evaluate your model on the held-out test dataset.\n",
        "\n",
        "By training on the combined train + validation dataset after you have finished tuning your model, you give it more training data, which should lead to an increase in the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d9j6SRcgtOi"
      },
      "outputs": [],
      "source": [
        "# Find the best-performing model that we have tested\n",
        "best_row = df_classics.iloc[-1, :].copy()\n",
        "\n",
        "# Get the model type and model parameters\n",
        "model_name = best_row['model_name']\n",
        "model_params = best_row['model_params']\n",
        "\n",
        "# Instantiate the model again using the parameters\n",
        "model = classic_model_names[model_name](**model_params)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrF9hIwtgtOi"
      },
      "outputs": [],
      "source": [
        "# Concatenate the train and validation datasets together\n",
        "X_train_new = np.concatenate((X_train, X_val), axis=0)\n",
        "y_train_new = pd.concat((y_train, y_val), axis=0)\n",
        "\n",
        "print(X_train_new.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S6AGYolgtOi"
      },
      "source": [
        "Finally, we can fit the model on the combined train + validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC7kSDiEgtOj"
      },
      "outputs": [],
      "source": [
        "ti = time()\n",
        "\n",
        "model.fit(X_train_new, y_train_new)\n",
        "\n",
        "dt = time() - ti\n",
        "print(f'Finished fitting best model, total time: {dt:0.2f} s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYWVVNnggtOj"
      },
      "source": [
        "## Testing the re-trained model on the test dataset\n",
        "\n",
        "After re-fitting the best model on the train+validation dataset, you can finally test it on the test dataset.\n",
        "**Remember:** you should only do this *once!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BIsxm3NgtOk"
      },
      "outputs": [],
      "source": [
        "y_act_test = y_test\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "r2, mae, rmse = evaluate_model(model, X_test, y_test)\n",
        "print(f'r2: {r2:0.4f}')\n",
        "print(f'mae: {mae:0.4f}')\n",
        "print(f'rmse: {rmse:0.4f}')\n",
        "\n",
        "plot = plot_pred_act(y_act_test, y_pred_test, model, reg_line=True, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN22IfHQgtOl"
      },
      "source": [
        "We see that our model achieves decent performance on the held-out test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS-FapnHgtOl"
      },
      "source": [
        "# Effect of train/validation/test dataset split\n",
        "\n",
        "Using different train/validation/test splits can dramatically affect your model performance, even for classical ML models.\n",
        "\n",
        "Here, we provide a little demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsjOHKelgtOl"
      },
      "outputs": [],
      "source": [
        "X_train_unscaled, y_train, formulae_train, skipped_train = generate_features(df_train, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
        "X_val_unscaled, y_val, formulae_val, skipped_val = generate_features(df_val, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
        "X_test_unscaled, y_test, formulae_test, skipped_test = generate_features(df_test, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWK5KJzGgtOm"
      },
      "outputs": [],
      "source": [
        "X_train_original = X_train_unscaled.copy()\n",
        "X_val = X_val_unscaled.copy()\n",
        "X_test = X_test_unscaled.copy()\n",
        "\n",
        "y_train_original = y_train.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhnG8FCYgtOm"
      },
      "source": [
        "We sample the training data using 10 random seeds, by using the `DataFrame.sample()` method with seeds ranging from 0 to 9.\n",
        "We then fit 10 models, each on one of the random splits, and evaluate their performance on the same validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftdAYNFRgtOm"
      },
      "outputs": [],
      "source": [
        "splits = range(10)\n",
        "df_splits = pd.DataFrame(columns=['split',\n",
        "                                  'r2_train',\n",
        "                                  'mae_train',\n",
        "                                  'rmse_train',\n",
        "                                  'r2_val',\n",
        "                                  'mae_val',\n",
        "                                  'rmse_val'])\n",
        "\n",
        "for split in splits:\n",
        "    print(f'Fitting and evaluating random split {split}')\n",
        "    X_train = X_train_original.sample(frac=0.7, random_state=split)\n",
        "    y_train = y_train_original[X_train.index]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = normalize(scaler.fit_transform(X_train))\n",
        "    X_val = normalize(scaler.transform(X_val_unscaled))\n",
        "    X_test = normalize(scaler.transform(X_test_unscaled))\n",
        "\n",
        "    model = AdaBoostRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_act_val = y_val\n",
        "    y_pred_val = model.predict(X_val)\n",
        "\n",
        "    r2_train, mae_train, rmse_train = evaluate_model(model, X_train, y_train)\n",
        "    r2_val, mae_val, rmse_val = evaluate_model(model, X_val, y_val)\n",
        "    result_dict = {\n",
        "        'split': split,\n",
        "        'r2_train': r2_train,\n",
        "        'mae_train': mae_train,\n",
        "        'rmse_train': rmse_train,\n",
        "        'r2_val': r2_val,\n",
        "        'mae_val': mae_val,\n",
        "        'rmse_val': rmse_val}\n",
        "\n",
        "    df_splits = append_result_df(df_splits, result_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jrs8OHbgtOo"
      },
      "outputs": [],
      "source": [
        "df_splits['split'] = df_splits['split'].astype(int)\n",
        "df_splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H33GoNIgtOp"
      },
      "source": [
        "We then plot the train and validation $r^2$ scores for each of the 10 models.\n",
        "\n",
        "Note the high variability in the r2_val score. In contrast, the variability in the r2_train score is comparatively lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4NuWuwugtOp"
      },
      "outputs": [],
      "source": [
        "df_splits.plot('split', ['r2_train', 'r2_val'], kind='bar')\n",
        "plt.title(f'Performance of {type(model).__name__}\\nwith {len(splits)} different data splits')\n",
        "plt.ylim((0.5, 1.0))\n",
        "plt.ylabel('$r^2$')\n",
        "plt.xlabel('Split #')\n",
        "plt.legend(loc='lower right', framealpha=0.9)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oyqw9uxgtOq"
      },
      "source": [
        "This effect is even more pronounced when we plot the mean abolute error (MAE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J6RDpzEgtOq"
      },
      "outputs": [],
      "source": [
        "df_splits.plot('split', ['mae_train', 'mae_val'], kind='bar')\n",
        "plt.title(f'Performance of {type(model).__name__}\\nwith {len(splits)} different data splits')\n",
        "plt.ylabel('MAE in $\\mathrm{C}_\\mathrm{p}$ (J / mol K)')\n",
        "plt.xlabel('Split #')\n",
        "plt.legend(loc='lower right', framealpha=0.9)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwadkHFygtOq"
      },
      "source": [
        "Therefore, typically the average value of all the scores are reported, as this gives a much more accurate estimate of how well the model actually performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swl4pc0ygtOq"
      },
      "outputs": [],
      "source": [
        "avg_r2_val = df_splits['r2_val'].mean()\n",
        "avg_mae_val = df_splits['mae_val'].mean()\n",
        "\n",
        "print(f'Average validation r2: {avg_r2_val:0.4f}')\n",
        "print(f'Average validation MAE: {avg_mae_val:0.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNpzCV2TgtOr"
      },
      "source": [
        "# Modeling using neural network / deep learning-based models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCkIw9GygtOr"
      },
      "source": [
        "In this notebook, we will cover how to implement a simple neural network for the modeling of heat capacity.\n",
        "\n",
        "We will load, prepare featurize, and scale/normalize the input datasets the same way as we did in the pervious notebook. For more information about the individual steps, you can consult that notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe3BEpTqgtOr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "from CBFV.cbfv.composition import generate_features\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Set a random seed to ensure reproducibility across runs\n",
        "RNG_SEED = 42\n",
        "np.random.seed(RNG_SEED)\n",
        "torch.manual_seed(RNG_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2O2LvjdgtOr"
      },
      "source": [
        "## Featurizing and scaling data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUVRsl_IgtOr"
      },
      "source": [
        "Nothing new here---same steps as we've done in the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5v9r3iTgtOr"
      },
      "outputs": [],
      "source": [
        "PATH = os.getcwd()\n",
        "train_path = os.path.join(PATH, 'data/cp_train.csv')\n",
        "val_path = os.path.join(PATH, 'data/cp_val.csv')\n",
        "test_path = os.path.join(PATH, 'data/cp_test.csv')\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_val = pd.read_csv(val_path)\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "print(f'df_train DataFrame shape: {df_train.shape}')\n",
        "print(f'df_val DataFrame shape: {df_val.shape}')\n",
        "print(f'df_test DataFrame shape: {df_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU0TeE4igtOs"
      },
      "source": [
        "Here we do not sub-sample the datasets into smaller datasets like we did in the previous notebook.\n",
        "Typically, the more data you have for neural networks, the better the networks will be able to train, and the better they will perform (as long as they are well-conditioned).\n",
        "\n",
        "Additionally, the performance of `PyTorch` is very good for modern computers, especially if you have a modern CUDA-capable graphics processing unit (GPU) such as an Nvidia GPU to accelerate the computations.\n",
        "Our dataset is small enough to fit into almost all modern computers or CUDA-capable GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XHEmbeDgtOs"
      },
      "outputs": [],
      "source": [
        "rename_dict = {'Cp': 'target'}\n",
        "df_train = df_train.rename(columns=rename_dict)\n",
        "df_val = df_val.rename(columns=rename_dict)\n",
        "df_test = df_test.rename(columns=rename_dict)\n",
        "\n",
        "X_train_unscaled, y_train, formulae_train, skipped_train = generate_features(df_train, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
        "X_val_unscaled, y_val, formulae_val, skipped_val = generate_features(df_val, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
        "X_test_unscaled, y_test, formulae_test, skipped_test = generate_features(df_test, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nvMSpubgtOs"
      },
      "outputs": [],
      "source": [
        "X_train_unscaled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOZBf85rgtOt"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train_unscaled)\n",
        "X_val = scaler.transform(X_val_unscaled)\n",
        "X_test = scaler.transform(X_test_unscaled)\n",
        "\n",
        "X_train = normalize(X_train)\n",
        "X_val = normalize(X_val)\n",
        "X_test = normalize(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQhEahgIgtOt"
      },
      "source": [
        "## Building a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVzm-MR1gtOt"
      },
      "source": [
        "This is where you get to be the architect, and design your own neural network!\n",
        "\n",
        "For sake of clarity (and to ensure that this tutorial runs on all the potatoes of this world), we will define a simple dense fully-connected neural network (which we will call `DenseNet`) as an example.\n",
        "\n",
        "The input layer of `DenseNet` accepts input data in the dimension of each row of the input data, which is equal to the number of features in our CBFV featurization scheme.\n",
        "In our particular example, when featurized using the `oliynyk` featurizer, the input dimension is 177 (it is the second dimension when you view `X_train.shape`).\n",
        "\n",
        "The output layer dimension of `DenseNet` is 1, because we want to predict one value (heat capacity).\n",
        "\n",
        "In addition, `DenseNet` can have one or more \"hidden layers\" that are attached between the input and output layers. These can be any arbitrary dimensions $>1$ you want to choose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRKh5aDegtOt"
      },
      "source": [
        "### Defining the network in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dELlhUEJgtOu"
      },
      "outputs": [],
      "source": [
        "class DenseNet(nn.Module):\n",
        "    \"\"\"\n",
        "    This implements a dynamically-built dense fully-connected neural network\n",
        "    with leaky ReLU activation and optional dropout.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_dims: int\n",
        "        Number of input features (required).\n",
        "    hidden_dims: list of ints\n",
        "        Number of hidden features, where each integer represents the number of\n",
        "        hidden features in each subsequent hidden linear layer (optional,\n",
        "        default=[64, 32]).\n",
        "    output_dims: int\n",
        "        Number of output features (optional, default=1).\n",
        "    dropout: float\n",
        "        the dropout value (optional, default=0.0).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_dims,\n",
        "                 hidden_dims=[64, 32],\n",
        "                 output_dims=1,\n",
        "                 dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dims = input_dims\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.output_dims = output_dims\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Build a sub-block of linear networks\n",
        "        def fc_block(in_dim, out_dim, *args, **kwargs):\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(in_dim, out_dim, *args, **kwargs),\n",
        "                nn.Dropout(p=self.dropout),\n",
        "                nn.LeakyReLU()\n",
        "                )\n",
        "\n",
        "        # Build overall network architecture\n",
        "        self.network = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(input_dims, self.hidden_dims[0]),\n",
        "                    nn.Dropout(p=self.dropout),\n",
        "                    nn.LeakyReLU())\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        hidden_layer_sizes = zip(self.hidden_dims[:-1], self.hidden_dims[1:])\n",
        "        self.network.extend([\n",
        "            fc_block(in_dim, out_dim) for in_dim, out_dim\n",
        "            in hidden_layer_sizes]\n",
        "            )\n",
        "\n",
        "        self.network.extend([\n",
        "            nn.Linear(hidden_dims[-1], output_dims)]\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the DenseNet model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.Tensor\n",
        "            A representation of the chemical compounds in the shape\n",
        "            (n_compounds, n_feats).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y: torch.Tensor\n",
        "            The element property prediction with the shape 1.\n",
        "        \"\"\"\n",
        "        for i, subnet in enumerate(self.network):\n",
        "            x = subnet(x)\n",
        "\n",
        "        y = x\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifEQEqdFgtOu"
      },
      "source": [
        "### Specifying the compute device for calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7pdN2i_gtOu"
      },
      "source": [
        "Before we run the neural network, we can first check if your machine has a CUDA-capable device.\n",
        "CUDA is a specific set of application instructions (application programming interfaces, APIs) that PyTorch can use to accelerate some of the calculations performed in neural networks.\n",
        "\n",
        "Generally, a relatively recent GPU from Nvidia will support CUDA capabilities, and can be used to accelerate neural network computations in PyTorch.\n",
        "\n",
        "In case you do not have a CUDA-capable device, PyTorch will fall back to using the CPU. Depending on the complexity of your model, training and predicting using a CPU can take significantly longer than using a CUDA-capable GPU.\n",
        "\n",
        "Consult the [PyTorch](https://pytorch.org/docs/stable/torch.html) and [CUDA](https://docs.nvidia.com/cuda/) documentation for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy-NW6zSgtOv"
      },
      "outputs": [],
      "source": [
        "CUDA_available = torch.cuda.is_available()\n",
        "print(f'CUDA is available: {CUDA_available}')\n",
        "\n",
        "if CUDA_available:\n",
        "    compute_device = torch.device('cuda')\n",
        "else:\n",
        "    compute_device = torch.device('cpu')\n",
        "\n",
        "print(f'Compute device for PyTorch: {compute_device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2vq58AogtOv"
      },
      "source": [
        "### Defining the data loader and dataset structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU5fAxODgtOv"
      },
      "source": [
        "Here we define a dataloader class specific for loading CBFV-type datasets.\n",
        "\n",
        "We also define the CBFV dataset class that tells PyTorch how our dataset is structured, and how to grab individual data samples from our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENSakGX3gtOw"
      },
      "outputs": [],
      "source": [
        "class CBFVDataLoader():\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_data: np.ndarray or pd.DataFrame or pd.Series\n",
        "        name of csv file containing cif and properties\n",
        "    val_data: np.ndarray or pd.DataFrame or pd.Series\n",
        "        name of csv file containing cif and properties\n",
        "    test_data: np.ndarray or pd.DataFrame or pd.Series\n",
        "        name of csv file containing cif and properties\n",
        "    batch_size: float, optional (default=64)\n",
        "        Step size for the Gaussian filter\n",
        "    random_state: int, optional (default=42)\n",
        "        Random seed for sampling the dataset. Only used if validation data is\n",
        "        not given.\n",
        "    shuffle: bool, optional (default=True)\n",
        "        Whether to shuffle the datasets or not\n",
        "    \"\"\"\n",
        "    def __init__(self, train_data, val_data, test_data,\n",
        "                 batch_size=64, num_workers=1, random_state=42,\n",
        "                 shuffle=True, pin_memory=True):\n",
        "\n",
        "        self.train_data = train_data\n",
        "        self.val_data = val_data\n",
        "        self.test_data = test_data\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "        self.shuffle = shuffle\n",
        "        self.random_state = random_state\n",
        "\n",
        "\n",
        "    def get_data_loaders(self, batch_size=1):\n",
        "        '''\n",
        "        Input the dataset, get train test split\n",
        "        '''\n",
        "        train_dataset = CBFVDataset(self.train_data)\n",
        "        val_dataset = CBFVDataset(self.val_data)\n",
        "        test_dataset = CBFVDataset(self.test_data)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset,\n",
        "                                  batch_size=self.batch_size,\n",
        "                                  pin_memory=self.pin_memory,\n",
        "                                  shuffle=self.shuffle)\n",
        "\n",
        "        val_loader = DataLoader(val_dataset,\n",
        "                                batch_size=self.batch_size,\n",
        "                                pin_memory=self.pin_memory,\n",
        "                                shuffle=self.shuffle)\n",
        "\n",
        "        test_loader = DataLoader(test_dataset,\n",
        "                                 batch_size=self.batch_size,\n",
        "                                 pin_memory=self.pin_memory,\n",
        "                                 shuffle=False)\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "class CBFVDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Get X and y from CBFV-based dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset):\n",
        "        self.data = dataset\n",
        "\n",
        "        self.X = np.array(self.data[0])\n",
        "        self.y = np.array(self.data[1])\n",
        "        self.shape = [(self.X.shape), (self.y.shape)]\n",
        "\n",
        "    def __str__(self):\n",
        "        string = f'CBFVDataset with X.shape {self.X.shape}'\n",
        "        return string\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X[[idx], :]\n",
        "        y = self.y[idx]\n",
        "\n",
        "        X = torch.as_tensor(X)\n",
        "        y = torch.as_tensor(np.array(y))\n",
        "\n",
        "        return (X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFNot_zSgtOw"
      },
      "source": [
        "Here we choose a batch size for loading data, and initialize the DataLoader for loading the featurized input data.\n",
        "\n",
        "We also get the data loaders corresponding to the train, validation, and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzISCiM2gtOw"
      },
      "outputs": [],
      "source": [
        "train_data = (X_train, y_train)\n",
        "val_data = (X_val, y_val)\n",
        "test_data = (X_test, y_test)\n",
        "\n",
        "# Instantiate the DataLoader\n",
        "batch_size = 128\n",
        "data_loaders = CBFVDataLoader(train_data, val_data, test_data, batch_size=batch_size)\n",
        "train_loader, val_loader, test_loader = data_loaders.get_data_loaders()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "picoRO8pgtOx"
      },
      "source": [
        "### Instantiating a `DenseNet` model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-fUqsMsgtOx"
      },
      "source": [
        "Now, we can instantiate... an instance of the `DenseNet` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKCe6qBfgtOx"
      },
      "outputs": [],
      "source": [
        "# Get input dimension size from the dataset\n",
        "example_data = train_loader.dataset.data[0]\n",
        "input_dims = example_data.shape[-1]\n",
        "\n",
        "# Instantiate the model\n",
        "model = DenseNet(input_dims, hidden_dims=[16], dropout=0.0)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI-OkdFYgtOx"
      },
      "source": [
        "### Defining the loss criterion & optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owta_tzigtOx"
      },
      "source": [
        "Here, we see the model and its individual layers and components printed nicely.\n",
        "\n",
        "We then instantiate and initialize the loss criterion and optimizer.\n",
        "\n",
        "Note, there are many choices of loss criteria and optimizers that are provided by PyTorch, each with their benefits and limitations, and a myriad of parameters.\n",
        "Consult the PyTorch documentation for further details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuN9W3z2gtOx"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss criterion\n",
        "criterion = nn.L1Loss()\n",
        "print('Loss criterion: ')\n",
        "print(criterion)\n",
        "\n",
        "# Initialize the optimzer\n",
        "optim_lr = 1e-2\n",
        "optimizer = optim.Adam(model.parameters(), lr=optim_lr)\n",
        "print('\\nOptimizer: ')\n",
        "print(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZvWI-_kgtOx"
      },
      "source": [
        "### Moving the model to the compute device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ndPQ9WOgtOy"
      },
      "source": [
        "Then, we can move the model and loss criterion computation to the compute device.\n",
        "\n",
        "If you have a GPU, this will trasnfer and attach the required resources to the GPU. If you have a CPU, then everything will remain on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld_kHDnqgtOy"
      },
      "outputs": [],
      "source": [
        "# Move the model and criterion to the compute device\n",
        "model = model.to(compute_device)\n",
        "criterion = criterion.to(compute_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "samNN1LsgtOy"
      },
      "source": [
        "### Defining some additional helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBcdstSbgtOy"
      },
      "source": [
        "We define some scaler functions and helper functions to evaluate and visualize model results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QrRHNuOgtOy"
      },
      "outputs": [],
      "source": [
        "class Scaler():\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.as_tensor(data)\n",
        "        self.mean = torch.mean(self.data)\n",
        "        self.std = torch.std(self.data)\n",
        "\n",
        "    def scale(self, data):\n",
        "        data = torch.as_tensor(data)\n",
        "        data_scaled = (data - self.mean) / self.std\n",
        "        return data_scaled\n",
        "\n",
        "    def unscale(self, data_scaled):\n",
        "        data_scaled = torch.as_tensor(data_scaled)\n",
        "        data = data_scaled * self.std + self.mean\n",
        "        return data\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {'mean': self.mean,\n",
        "                'std': self.std}\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.mean = state_dict['mean']\n",
        "        self.std = state_dict['std']\n",
        "\n",
        "\n",
        "class MeanLogNormScaler():\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.as_tensor(data)\n",
        "        self.logdata = torch.log(self.data)\n",
        "        self.mean = torch.mean(self.logdata)\n",
        "        self.std = torch.std(self.logdata)\n",
        "\n",
        "    def scale(self, data):\n",
        "        data = torch.as_tensor(data)\n",
        "        data_scaled = (torch.log(data) - self.mean) / self.std\n",
        "        return data_scaled\n",
        "\n",
        "    def unscale(self, data_scaled):\n",
        "        data_scaled = torch.as_tensor(data_scaled) * self.std + self.mean\n",
        "        data = torch.exp(data_scaled)\n",
        "        return data\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {'mean': self.mean,\n",
        "                'std': self.std}\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.mean = state_dict['mean']\n",
        "        self.std = state_dict['std']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_5JrkgSgtOy"
      },
      "outputs": [],
      "source": [
        "def predict(model, data_loader):\n",
        "    target_list = []\n",
        "    pred_list = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data_output in enumerate(data_loader):\n",
        "            X, y_act = data_output\n",
        "            X = X.to(compute_device,\n",
        "                     dtype=data_type,\n",
        "                     non_blocking=True)\n",
        "            y_act = y_act.cpu().flatten().tolist()\n",
        "            y_pred = model.forward(X).cpu().flatten().tolist()\n",
        "\n",
        "            # Unscale target values\n",
        "            y_pred = target_scaler.unscale(y_pred).tolist()\n",
        "\n",
        "            targets = y_act\n",
        "            predictions = y_pred\n",
        "            target_list.extend(targets)\n",
        "            pred_list.extend(predictions)\n",
        "    model.train()\n",
        "\n",
        "    return target_list, pred_list\n",
        "\n",
        "\n",
        "def evaluate(target, pred):\n",
        "    r2 = r2_score(target, pred)\n",
        "    mae = mean_absolute_error(target, pred)\n",
        "    rmse = mean_squared_error(target, pred, squared=False)\n",
        "    output = (r2, mae, rmse)\n",
        "    return output\n",
        "\n",
        "\n",
        "def print_scores(scores, label=''):\n",
        "    r2, mae, rmse = scores\n",
        "    print(f'{label} r2: {r2:0.4f}')\n",
        "    print(f'{label} mae: {mae:0.4f}')\n",
        "    print(f'{label} rmse: {rmse:0.4f}')\n",
        "    return scores\n",
        "\n",
        "\n",
        "def plot_pred_act(act, pred, model, reg_line=True, label=''):\n",
        "    xy_max = np.max([np.max(act), np.max(pred)])\n",
        "\n",
        "    plot = plt.figure(figsize=(6,6))\n",
        "    plt.plot(act, pred, 'o', ms=9, mec='k', mfc='silver', alpha=0.4)\n",
        "    plt.plot([0, xy_max], [0, xy_max], 'k--', label='ideal')\n",
        "    if reg_line:\n",
        "        polyfit = np.polyfit(act, pred, deg=1)\n",
        "        reg_ys = np.poly1d(polyfit)(np.unique(act))\n",
        "        plt.plot(np.unique(act), reg_ys, alpha=0.8, label='linear fit')\n",
        "    plt.axis('scaled')\n",
        "    plt.xlabel(f'Actual {label}')\n",
        "    plt.ylabel(f'Predicted {label}')\n",
        "    plt.title(f'{type(model).__name__}, r2: {r2_score(act, pred):0.4f}')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    return plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaJplvN5gtOz"
      },
      "source": [
        "We scale the target variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyhZZ_qugtOz"
      },
      "outputs": [],
      "source": [
        "y_train = [data[1].numpy().tolist() for data in train_loader]\n",
        "y_train = [item for sublist in y_train for item in sublist]\n",
        "\n",
        "y_train = train_loader.dataset.y\n",
        "\n",
        "target_scaler = MeanLogNormScaler(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48jX7lm7gtOz"
      },
      "source": [
        "### Training the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZWOxaqigtOz"
      },
      "source": [
        "And finally, we train the neural network.\n",
        "\n",
        "This is the training procedure for the neural network:\n",
        "* for each `epoch`:\n",
        "  * iterate through the train dataset using `train_loader`:\n",
        "    * scale the target data\n",
        "    * transfer input (`X`) and target (`y`) data to compute device\n",
        "    * reset the optimizer's gradient to zero\n",
        "    * compute the output of the model (forward pass)\n",
        "    * calculate the loss of the model (between the predicted and true target values)\n",
        "    * propagate the loss backwards through the model (backpropagation)\n",
        "    * update the weights throughout the model\n",
        "  * if `epoch == print_every`:\n",
        "    * print the current epoch (to keep track of training progress)\n",
        "  * if `epoch == plot_every`:\n",
        "    * evaluate the model on the validation dataset using `val_loader`\n",
        "    * plot predicted vs. actual value plots\n",
        "    * print the train and val $r^2$, $\\textrm{MAE}$ and $\\textrm{RMSE}$ scores of the model\n",
        "\n",
        "Note: training this network may take up to tens of minutes, depending on your hardware configuration and whether or not you have a CUDA-capable device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hp90uJcgtOz"
      },
      "outputs": [],
      "source": [
        "data_type = torch.float\n",
        "epochs = 500\n",
        "\n",
        "print_every = 20\n",
        "plot_every = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    if epoch % print_every == 0 or epoch == epochs - 1:\n",
        "        print(f'epoch: {epoch}')\n",
        "    if epoch % plot_every == 0:\n",
        "        target_train, pred_train = predict(model, train_loader)\n",
        "        train_scores = evaluate(target_train, pred_train)\n",
        "        print_scores(train_scores, label='train')\n",
        "\n",
        "        target_val, pred_val = predict(model, val_loader)\n",
        "        val_scores = evaluate(target_val, pred_val)\n",
        "        print_scores(val_scores, label='val')\n",
        "        plot_pred_act(target_val, pred_val, model, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')\n",
        "        plt.show()\n",
        "\n",
        "    for i, data_output in enumerate(train_loader):\n",
        "        X, y = data_output\n",
        "        y = target_scaler.scale(y)\n",
        "\n",
        "        X = X.to(compute_device,\n",
        "                 dtype=data_type,\n",
        "                 non_blocking=True)\n",
        "        y = y.to(compute_device,\n",
        "                 dtype=data_type,\n",
        "                 non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(X).flatten()\n",
        "        loss = criterion(output.view(-1), y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTG_xW99gtO0"
      },
      "source": [
        "Now, with our trained neural network, we can evaluate the performance of the model (at the end of the training phase) on the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJaYiCWMgtO0"
      },
      "outputs": [],
      "source": [
        "target_val, pred_val = predict(model, val_loader)\n",
        "scores = evaluate(target_val, pred_val)\n",
        "\n",
        "print_scores(scores, label='val')\n",
        "\n",
        "plot = plot_pred_act(target_val, pred_val, model, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtgMR-D3gtO0"
      },
      "source": [
        "## Keeping track of training progress -- avoid overfitting\n",
        "\n",
        "Note, you can keep track of the training progress by saving the train and validation metrics such as $r^2$ and MAE at every epoch.\n",
        "Then, you can plot so-called \"loss curves\" that show the loss of the model vs. epoch throughout the training process.\n",
        "This gives you additional insight into your model training process, and helps you diagnose issues such as overfitting, improper model/optimizer/loss parameters, and so on.\n",
        "\n",
        "Once you start tracking these performance metrics during your training loop, you can also implement more advanced training techniques such as \"early stopping\".\n",
        "In early stopping, you observe the performance metrics (such as validation $r^2$ or MAE) over the training epochs, and you stop the training process if you observe that the metrics are not improving any more (meaning your model is fully trained), or if the metrics are increasing again after reaching a minimum (meaning your model is overfitting the training set).\n",
        "\n",
        "## Evaluating model performance on test dataset\n",
        "\n",
        "And finally evaluate the performance on the test dataset.\n",
        "**Remember:** you should only do this *once!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsHRXG_ogtO0"
      },
      "outputs": [],
      "source": [
        "target_test, pred_test = predict(model, test_loader)\n",
        "scores = evaluate(target_test, pred_test)\n",
        "\n",
        "print_scores(scores, label='test')\n",
        "\n",
        "plot = plot_pred_act(target_test, pred_test, model, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLHg9IsKgtO1"
      },
      "source": [
        "# Exporting PyTorch models\n",
        "\n",
        "Now that we've got a (reasonably well-performing) model, we can export the weights and biases from the model to what is referred to as a \"checkpoint\" file.\n",
        "\n",
        "The advantages of exporting your model to a checkpoint file are manifold.\n",
        "For example, when you want to re-use the model again later (to make further predictions, or even to continue training), you don't have to train the model from scratch again.\n",
        "For our current `DenseNet` model, this may not seem like a big deal, since it trains within minutes.\n",
        "But once you start moving on to larger and larger models, model training time can reach hours, days---even weeks!\n",
        "\n",
        "Another advantage is that you can greatly enhance the reproducibility of your work.\n",
        "If you export your models, other researchers can then recreate your model architecture on their system, then load your weights into the model to get exactly the model you trained.\n",
        "This allows them to use your model as-is, and enables them to reproduce your work---an important step if they are to judge the merit of your work.\n",
        "\n",
        "With that said, we will now use PyTorch's built-in methods to export (1) our `DenseNet` model, and (2) our `target_scaler` (we need to export our `target_scaler` object as well, because we need to use it to unscale the model predictions to get back the true prediction values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnf1EWQ5gtO1"
      },
      "source": [
        "## Saving the model + target scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4iWFIUHgtO1"
      },
      "outputs": [],
      "source": [
        "save_dict = {'weights': model.state_dict(),\n",
        "             'scaler_state': target_scaler.state_dict()}\n",
        "print(save_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZTS8ssjgtO1"
      },
      "outputs": [],
      "source": [
        "pth_path = ('model_checkpoint.pth') # .pth is commonly used as the file extension\n",
        "torch.save(save_dict, pth_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPhfa-MsgtO1"
      },
      "source": [
        "Navigate to your notebooks directory. You should now find a file named 'model_checkpoint.pth'.\n",
        "Since the `DenseNet` model is small, the checkpoint file weighs in relatively lightly at 13KB.\n",
        "Bigger models will have more weights & biases, and will require more storage space for the checkpoint file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7tD6rDqgtO1"
      },
      "source": [
        "## Loading the model + target scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI12eCk0gtO2"
      },
      "source": [
        "Of course, if you provide the facilities to **save** a model, you should also provide facilities to **load** them and to recreate your model back.\n",
        "\n",
        "Thankfully, PyTorch makes this also easy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teTb5LD7gtO2"
      },
      "outputs": [],
      "source": [
        "# First, clear the variables for model and target_scaler.\n",
        "# We want to start with a clean slate.\n",
        "model = None\n",
        "target_scaler = None\n",
        "del model\n",
        "del target_scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RdpkcmXgtO2"
      },
      "source": [
        "We start by recreating the `DenseNet` model and the `target_scaler` that we originally built. This model will be initialized with random weights & biases, which we will then overload (overwrite) afterwards with the values from the checkpoint file.\n",
        "\n",
        "Make sure that you create the same model and `target_scaler` here as the ones you saved the checkpoint file from. Otherwise you will not be able to load the checkpoint file, or it will produce unexpected results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wYuZ3D1gtO2"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model.\n",
        "# The model will be randomly initialized, but we will overwrite\n",
        "# all weights and biases when we load the checkpoint.\n",
        "model = DenseNet(input_dims, hidden_dims=[16], dropout=0.0)\n",
        "model = model.to(compute_device)\n",
        "print(model)\n",
        "\n",
        "# Instantiate the target_scaler.\n",
        "# We initialize this target_scaler with a vector of zeros,\n",
        "# but we will overwrite its internal parameters\n",
        "# when we load the checkpoint.\n",
        "target_scaler = MeanLogNormScaler(torch.zeros(42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onTjG3iRgtO2"
      },
      "outputs": [],
      "source": [
        "# Load the checkpoint and map it to the compute device\n",
        "pth_path = ('model_checkpoint.pth')\n",
        "checkpoint = torch.load(pth_path, map_location=compute_device)\n",
        "\n",
        "# Load the state dictionaries back into the model and target_scaler\n",
        "model.load_state_dict(checkpoint['weights'])\n",
        "target_scaler.load_state_dict(checkpoint['scaler_state'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzyvn91-gtO3"
      },
      "source": [
        "## Checking the loaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvjulvE8gtO4"
      },
      "outputs": [],
      "source": [
        "target_test, pred_test = predict(model, test_loader)\n",
        "scores = evaluate(target_test, pred_test)\n",
        "\n",
        "print_scores(scores, label='test')\n",
        "\n",
        "plot = plot_pred_act(target_test, pred_test, model, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68XCUlS1gtO4"
      },
      "source": [
        "Hooray!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOmAbN70gtO5"
      },
      "source": [
        "# Visualizing results\n",
        "\n",
        "Here, we will show some typical examples of visualizations that are used often to show results in ML studies in materials science.\n",
        "\n",
        "We will use the open-source [`ML_figures` package](https://github.com/kaaiian/ML_figures) and the example data provided by the package to generate these figures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAPXqfHugtO5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "# Import the ML_figures package and the figure-plotting functions\n",
        "from ML_figures.figures import act_pred\n",
        "from ML_figures.figures import residual, residual_hist\n",
        "from ML_figures.figures import loss_curve\n",
        "from ML_figures.figures import element_prevalence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjugED6KgtO5"
      },
      "source": [
        "## Predicted vs. actual value plots\n",
        "\n",
        "These plots, you have already seen before in the previous notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKgTtsrjgtO5"
      },
      "outputs": [],
      "source": [
        "# Read in example act vs. pred data\n",
        "df_act_pred = pd.read_csv('ML_figures/example_data/act_pred.csv')\n",
        "y_act, y_pred = df_act_pred.iloc[:, 1], df_act_pred.iloc[:, 2]\n",
        "\n",
        "act_pred(y_act, y_pred,\n",
        "         reg_line=True,\n",
        "         save_dir='ML_figures/example_figures')\n",
        "\n",
        "act_pred(y_act, y_pred,\n",
        "         name='example_no_hist',\n",
        "         x_hist=False, y_hist=False,\n",
        "         reg_line=True,\n",
        "         save_dir='ML_figures/example_figures')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHukLJi8gtO6"
      },
      "source": [
        "## Residual error plots\n",
        "\n",
        "Residual error plots show how far your model's predictions deviate from the actual values.\n",
        "They are using the same data used in the predicted vs. actual plots; however, instead of plotting predicted vs. actual values, residual error plots plot (predicted - actual) vs. actual values.\n",
        "\n",
        "This lets you visually analyze your model's prediction error on a straight horizontal line.\n",
        "\n",
        "Alternatively, you can plot the residual errors on a histogram, and optionally with a kernel density estimation (kde)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3faxj12kgtO6"
      },
      "outputs": [],
      "source": [
        "residual(y_act, y_pred,\n",
        "         save_dir='ML_figures/example_figures')\n",
        "\n",
        "residual_hist(y_act, y_pred,\n",
        "              save_dir='ML_figures/example_figures')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZHxoH5OgtO6"
      },
      "source": [
        "## Loss curves\n",
        "\n",
        "Loss curves show the loss of a neural network model vs. epoch throughout the training process.\n",
        "It is typically evaluated using the training and validation dataset at the end of each epoch (or every $n$ epochs, where $n$ is a small number, if evaluating every epoch takes too many resources).\n",
        "\n",
        "Typically, loss curves plot the model performance (such as $r^2$ score) or loss (such as $\\textrm{MAE}$) against epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTnzUIe0gtO6"
      },
      "outputs": [],
      "source": [
        "# Read in loss curve data\n",
        "df_lc = pd.read_csv('ML_figures/example_data/training_progress.csv')\n",
        "epoch = df_lc['epoch']\n",
        "train_err, val_err = df_lc['mae_train'], df_lc['mae_val']\n",
        "\n",
        "loss_curve(epoch, train_err, val_err,\n",
        "           save_dir='ML_figures/example_figures')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_R-WzMjgtO6"
      },
      "source": [
        "## Visualizing elemental prevalence\n",
        "\n",
        "Depending on your dataset, what you are studying, and how the compounds/constituent elements of the compounds in the dataset are distributed, it may be useful to visualize the elemental prevalence in your dataset.\n",
        "\n",
        "These figures let you visualize the relative amount of certain elements vs. other elements present in your dataset, and can help you in identifying dataset biases, imbalanced datasets, or other issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRl4BIengtO7"
      },
      "outputs": [],
      "source": [
        "# Visualize element prevalence\n",
        "formula = df_act_pred.iloc[:, 0]\n",
        "\n",
        "element_prevalence(formula,\n",
        "                   save_dir='ML_figures/example_figures',\n",
        "                   log_scale=False)\n",
        "element_prevalence(formula,\n",
        "                   save_dir='ML_figures/example_figures',\n",
        "                   name='example_log',\n",
        "                   log_scale=True)\n",
        "\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "element_prevalence(formula,\n",
        "                   save_dir='ML_figures/example_figures',\n",
        "                   ptable_fig=False,\n",
        "                   log_scale=False)\n",
        "element_prevalence(formula,\n",
        "                   save_dir='ML_figures/example_figures',\n",
        "                   name='example_log',\n",
        "                   ptable_fig=False,\n",
        "                   log_scale=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "course-book",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "89d7d07a18bc6bcfb9fd99f9d58ef662eeaa9759129ecdb773c42f07a9c126b5"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}