{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Featurization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will show some simple examples of featurizing materials composition data using so-called \"composition-based feature vectors\", or CBFVs. This methods represents a single chemical formula as one vector based on its constituent atoms' chemical properties (refer to the paper for more information and references).\n",
    "\n",
    "Note that the steps shown in this notebook are intended to demonstrate the best practices associated with featurizing materials data, using *one* way of featurizing materials composition data as an example. \n",
    "Depending on your input data and your particular modeling needs, the data featurization method and procedure you use may be different than the example shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Set a random seed to ensure reproducibility across runs\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "We will start with the dataset splits that we saved from the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "train_path = os.path.join(PATH, 'data/cp_train.csv')\n",
    "val_path = os.path.join(PATH, 'data/cp_val.csv')\n",
    "test_path = os.path.join(PATH, 'data/cp_test.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_val = pd.read_csv(val_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(f'df_train DataFrame shape: {df_train.shape}')\n",
    "print(f'df_val DataFrame shape: {df_val.shape}')\n",
    "print(f'df_test DataFrame shape: {df_test.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-sampling your data (optional)\n",
    "\n",
    "If your dataset is too large, you can subsample it to be a smaller size.\n",
    "This is useful for prototyping and for making quick sanity tests of new models / parameters.\n",
    "\n",
    "Just be aware that you do not introduce any bias into your data through the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-sample the data. Set the random_state to make the sampling reproducible every time.\n",
    "df_train_sampled = df_train.sample(n=2000, random_state=RNG_SEED)\n",
    "df_val_sampled = df_val.sample(n=200, random_state=RNG_SEED)\n",
    "df_test_sampled = df_test.sample(n=200, random_state=RNG_SEED)\n",
    "\n",
    "print(f'df_train_sampled DataFrame shape: {df_train_sampled.shape}')\n",
    "print(f'df_val_sampled DataFrame shape: {df_val_sampled.shape}')\n",
    "print(f'df_test_sampled DataFrame shape: {df_test_sampled.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features using the `CBFV` package\n",
    "\n",
    "To featurize the chemical compositions from a chemical formula (e.g. \"Al2O3\") into a composition-based feature vector (CBFV), we use the open-source [`CBFV` package](https://github.com/kaaiian/CBFV).\n",
    "\n",
    "We have downloaded and saved a local copy of the package into this repository for your convenience.\n",
    "For the most updated version, refer to the GitHub repository linked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package and the generate_features function\n",
    "from CBFV.cbfv.composition import generate_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate_features` function from the CBFV package expects an input DataFrame containing at least the columns `['formula', 'target']`. You may also have extra feature columns (e.g., `temperature` or `pressure`, other measurement conditions, etc.).\n",
    "\n",
    "In our dataset, `Cp` represents the target variable, and `T` is the measurement condition.\n",
    "Since the `generate_features` function expects the target variable column to be named `target`, we have to rename the `Cp` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DataFrame column names before renaming:')\n",
    "print(df_train.columns)\n",
    "print(df_val.columns)\n",
    "print(df_test.columns)\n",
    "\n",
    "rename_dict = {'Cp': 'target'}\n",
    "df_train = df_train.rename(columns=rename_dict)\n",
    "df_val = df_val.rename(columns=rename_dict)\n",
    "df_test = df_test.rename(columns=rename_dict)\n",
    "\n",
    "df_train_sampled = df_train_sampled.rename(columns=rename_dict)\n",
    "df_val_sampled = df_val_sampled.rename(columns=rename_dict)\n",
    "df_test_sampled = df_test_sampled.rename(columns=rename_dict)\n",
    "\n",
    "print('\\nDataFrame column names after renaming:')\n",
    "print(df_train.columns)\n",
    "print(df_val.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `generate_features` function to generate the CBFVs from the input data.\n",
    "\n",
    "Note that we have specified several keyword arguments in our call to `generate_features`:\n",
    "* `elem_prop='oliynyk'`\n",
    "* `drop_duplicates=False`\n",
    "* `extend_features=True`\n",
    "* `sum_feat=True`\n",
    "\n",
    "A short explanation for the choice of keyword arguments is below:\n",
    "* The `elem_prop` parameter specifies which CBFV featurization scheme to use (there are several). For this tutorial, we have chosen to use the `oliynyk` CBFV featurization scheme.\n",
    "* The `drop_duplicates` parameter specifies whether to drop duplicate formulae during featurization. In our case, we want to preserve duplicate formulae in our data (`True`), since we have multiple heat capacity measurements (performed at different temperatures) for the same compound.\n",
    "* The `extend_features` parameter specifies whether to include extended features (features that are not part of `['formula', 'target']`) in the featurized data. In our case, this is our measurement temperature, and we want to include this information (`True`), since this is pertinent information for the heat capacity prediction.\n",
    "* The `sum_feat` parameter specifies whether to calculate the sum features when generating the CBFVs for the chemical formulae. We do in our case (`True`).\n",
    "\n",
    "For more information about the `generate_features` function and the CBFV featurization scheme, refer to the GitHub repository and the accompanying paper to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled, y_train, formulae_train, skipped_train = generate_features(df_train_sampled, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_val_unscaled, y_val, formulae_val, skipped_val = generate_features(df_val_sampled, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test_unscaled, y_test, formulae_test, skipped_test = generate_features(df_test_sampled, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what a featurized X matrix looks like, `.head()` will show us some rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `sum` features in the CBFV, which we have included by using `sum_feat=True` in the call to `generate_features`.\n",
    "\n",
    "Also note the temperature column `T` at the end of this featurized data.\n",
    "\n",
    "What we have done above is featurize the input data. In the featurized data, each row contains a unique CBFV that describes a given chemical composition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaling & normalization\n",
    "\n",
    "For numerical input data, scaling and normalization of the features often improves the model performance.\n",
    "Scaling can partially correct the discrepancy between the orders of magnitudes of the features (e.g., some numerical features being much larger or smaller than others).\n",
    "This typically improves the model learning performance, and in turn, improves the model performance.\n",
    "\n",
    "We will scale then normalize our input data using scikit-learn's built-in `StandardScaler` class and `normalize` function.\n",
    "\n",
    "Note, in addition to `StandardScaler`, other scalers such as `RobustScaler` and `MinMaxScaler` are also available in scikit-learn. Consult the documentation for the details and when to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data\n",
    "\n",
    "First, we instantiate the scaler object.\n",
    "\n",
    "In a `StandardScaler` object:\n",
    "* During the `fit` process, the statistics of the input data (mean and standard deviation) are computed.\n",
    "* Then, during the `transform` process, the mean and standard deviation values calculated above are used to scale the data to having zero-mean and unit variance.\n",
    "\n",
    "Therefore, for the first time usage of the scaler, we call the `.fit_transform()` method to fit the scaler to the input data, and then to transform the same data.\n",
    "For subsequent uses, since we have already computed the statistics, we only call the `.transform()` method to scale data.\n",
    "\n",
    "**Note:** you should *only* `.fit()` the scaler using the training dataset statistics, and then use these same statistics from the training dataset to `.transform()` the other datasets (validation and train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train_unscaled)\n",
    "X_val = scaler.transform(X_val_unscaled)\n",
    "X_test = scaler.transform(X_test_unscaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the scaled data\n",
    "\n",
    "We repeat a similar process for normalizing the data.\n",
    "Here, there is no need to first fit the normalizer, since the normalizer scales the rows of the input data to unit norm independently of other rows.\n",
    "\n",
    "The normalizer is different to a Scaler in that the normalizer acts row-wise, whereas a Scaler acts column-wise on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train)\n",
    "X_val = normalize(X_val)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling using \"classical\" machine learning models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement some classical ML models from `sklearn`:\n",
    "\n",
    "* Ridge regression\n",
    "* Support vector machine\n",
    "* Linear support vector machine\n",
    "* Random forest\n",
    "* Extra trees\n",
    "* Adaptive boosting\n",
    "* Gradient boosting\n",
    "* k-nearest neighbors\n",
    "* Dummy (if you can't beat this, something is wrong.)\n",
    "\n",
    "Note: the Dummy model types from `sklearn` act as a good sanity check for your ML studies. If your models do not perform significantly better than the equivalent Dummy models, then you should know that something has gone wrong in your model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_model(model_name):\n",
    "    model = model_name()\n",
    "    return model\n",
    "\n",
    "def fit_model(model, X_train, y_train):\n",
    "    ti = time()\n",
    "    model = instantiate_model(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_time = time() - ti\n",
    "    return model, fit_time\n",
    "\n",
    "def evaluate_model(model, X, y_act):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y_act, y_pred)\n",
    "    mae = mean_absolute_error(y_act, y_pred)\n",
    "    rmse_val = mean_squared_error(y_act, y_pred, squared=False)\n",
    "    return r2, mae, rmse_val\n",
    "\n",
    "def fit_evaluate_model(model, model_name, X_train, y_train, X_val, y_act_val):\n",
    "    model, fit_time = fit_model(model, X_train, y_train)\n",
    "    r2_train, mae_train, rmse_train = evaluate_model(model, X_train, y_train)\n",
    "    r2_val, mae_val, rmse_val = evaluate_model(model, X_val, y_act_val)\n",
    "    result_dict = {\n",
    "        'model_name': model_name,\n",
    "        'model_name_pretty': type(model).__name__,\n",
    "        'model_params': model.get_params(),\n",
    "        'fit_time': fit_time,\n",
    "        'r2_train': r2_train,\n",
    "        'mae_train': mae_train,\n",
    "        'rmse_train': rmse_train,\n",
    "        'r2_val': r2_val,\n",
    "        'mae_val': mae_val,\n",
    "        'rmse_val': rmse_val}\n",
    "    return model, result_dict\n",
    "\n",
    "def append_result_df(df, result_dict):\n",
    "    df_result_appended = df.append(result_dict, ignore_index=True)\n",
    "    return df_result_appended\n",
    "\n",
    "def append_model_dict(dic, model_name, model):\n",
    "    dic[model_name] = model\n",
    "    return dic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an empty DataFrame to store model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classics = pd.DataFrame(columns=['model_name',\n",
    "                                    'model_name_pretty',\n",
    "                                    'model_params',\n",
    "                                    'fit_time',\n",
    "                                    'r2_train',\n",
    "                                    'mae_train',\n",
    "                                    'rmse_train',\n",
    "                                    'r2_val',\n",
    "                                    'mae_val',\n",
    "                                    'rmse_val'])\n",
    "df_classics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the models\n",
    "\n",
    "Here, we instantiate several classical machine learning models for use.\n",
    "For demonstration purposes, we instantiate the models with their default model parameters.\n",
    "\n",
    "Some of the models listed above can perform either regression or classification tasks.\n",
    "Because our ML task is a regression task (prediction of the continuous-valued target, heat capacity), we choose the regression variant of these models.\n",
    "\n",
    "Note: the `DummyRegressor()` instance acts as a good sanity check for your ML studies. If your models do not perform significantly better than the `DummyRegressor()`, then you know something has gone awry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary of model names\n",
    "classic_model_names = OrderedDict({\n",
    "    'dumr': DummyRegressor,\n",
    "    'rr': Ridge,\n",
    "    'abr': AdaBoostRegressor,\n",
    "    'gbr': GradientBoostingRegressor,\n",
    "    'rfr': RandomForestRegressor,\n",
    "    'etr': ExtraTreesRegressor,\n",
    "    'svr': SVR,\n",
    "    'lsvr': LinearSVR,\n",
    "    'knr': KNeighborsRegressor,\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and fit the models\n",
    "\n",
    "Now, we can fit the ML models.\n",
    "\n",
    "We will loop through each of the models listed above. For each of the models, we will:\n",
    "* instantiate the model (with default parameters)\n",
    "* fit the model using the training data\n",
    "* use the fitted model to generate predictions from the validation data\n",
    "* evaluate the performance of the model using the predictions\n",
    "* store the results in a DataFrame for analysis\n",
    "\n",
    "Note: this may take several minutes, depending on your hardware/software environment, dataset size and featurization scheme (CBFV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a dictionary to store the model objects\n",
    "classic_models = OrderedDict()\n",
    "\n",
    "# Keep track of elapsed time\n",
    "ti = time()\n",
    "\n",
    "# Loop through each model type, fit and predict, and evaluate and store results\n",
    "for model_name, model in classic_model_names.items():\n",
    "    print(f'Now fitting and evaluating model {model_name}: {model.__name__}')\n",
    "    model, result_dict = fit_evaluate_model(model, model_name, X_train, y_train, X_val, y_val)\n",
    "    df_classics = append_result_df(df_classics, result_dict)\n",
    "    classic_models = append_model_dict(classic_models, model_name, model)\n",
    "\n",
    "dt = time() - ti\n",
    "print(f'Finished fitting {len(classic_models)} models, total time: {dt:0.2f} s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at the results.\n",
    "\n",
    "You will notice, that some of the models (such as RandomForestRegressor, ExtraTreesRegressor and GradientBoostingRegressor) have completely memorized the training data, as evidenced by the very high r2_train scores of ~1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort in order of increasing validation r2 score\n",
    "df_classics = df_classics.sort_values('r2_val', ignore_index=True)\n",
    "df_classics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now also access the full details of the models by inspecting the `classic_models` dictionary that we populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model performance on validation dataset\n",
    "\n",
    "Now comes the time to evaluate the trained models on the validation set.\n",
    "\n",
    "Remember, we use the same validation set to evaluate all models. This ensures a fair comparison.\n",
    "\n",
    "In addition, we plot the predicted vs. actual plots using the predictions made by each trained model on the same validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_act(act, pred, model, reg_line=True, label=''):\n",
    "    xy_max = np.max([np.max(act), np.max(pred)])\n",
    "\n",
    "    plot = plt.figure(figsize=(6,6))\n",
    "    plt.plot(act, pred, 'o', ms=9, mec='k', mfc='silver', alpha=0.4)\n",
    "    plt.plot([0, xy_max], [0, xy_max], 'k--', label='ideal')\n",
    "    if reg_line:\n",
    "        polyfit = np.polyfit(act, pred, deg=1)\n",
    "        reg_ys = np.poly1d(polyfit)(np.unique(act))\n",
    "        plt.plot(np.unique(act), reg_ys, alpha=0.8, label='linear fit')\n",
    "    plt.axis('scaled')\n",
    "    plt.xlabel(f'Actual {label}')\n",
    "    plt.ylabel(f'Predicted {label}')\n",
    "    plt.title(f'{type(model).__name__}, r2: {r2_score(act, pred):0.4f}')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(df_classics.shape[0]):\n",
    "    model_name = df_classics.iloc[row]['model_name']\n",
    "\n",
    "    model = classic_models[model_name]\n",
    "    y_act_val = y_val\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    plot = plot_pred_act(y_act_val, y_pred_val, model, reg_line=True, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-training the best-performing model on combined train + validation dataset\n",
    "\n",
    "After you have finalized your model, you can re-train your model (using the same hyperparameters) again on the combined train + validation datasets, and finally, evaluate your model on the held-out test dataset.\n",
    "\n",
    "By training on the combined train + validation dataset after you have finished tuning your model, you give it more training data, which should lead to an increase in the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best-performing model that we have tested\n",
    "best_row = df_classics.iloc[-1, :].copy()\n",
    "\n",
    "# Get the model type and model parameters\n",
    "model_name = best_row['model_name']\n",
    "model_params = best_row['model_params']\n",
    "\n",
    "# Instantiate the model again using the parameters\n",
    "model = classic_model_names[model_name](**model_params)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the train and validation datasets together\n",
    "X_train_new = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train_new = pd.concat((y_train, y_val), axis=0)\n",
    "\n",
    "print(X_train_new.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can fit the model on the combined train + validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = time()\n",
    "\n",
    "model.fit(X_train_new, y_train_new)\n",
    "\n",
    "dt = time() - ti\n",
    "print(f'Finished fitting best model, total time: {dt:0.2f} s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the re-trained model on the test dataset\n",
    "\n",
    "After re-fitting the best model on the train+validation dataset, you can finally test it on the test dataset.\n",
    "**Remember:** you should only do this *once!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_act_test = y_test\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "r2, mae, rmse = evaluate_model(model, X_test, y_test)\n",
    "print(f'r2: {r2:0.4f}')\n",
    "print(f'mae: {mae:0.4f}')\n",
    "print(f'rmse: {rmse:0.4f}')\n",
    "\n",
    "plot = plot_pred_act(y_act_test, y_pred_test, model, reg_line=True, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model achieves decent performance on the held-out test dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of train/validation/test dataset split\n",
    "\n",
    "Using different train/validation/test splits can dramatically affect your model performance, even for classical ML models.\n",
    "\n",
    "Here, we provide a little demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled, y_train, formulae_train, skipped_train = generate_features(df_train, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_val_unscaled, y_val, formulae_val, skipped_val = generate_features(df_val, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test_unscaled, y_test, formulae_test, skipped_test = generate_features(df_test, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original = X_train_unscaled.copy()\n",
    "X_val = X_val_unscaled.copy()\n",
    "X_test = X_test_unscaled.copy()\n",
    "\n",
    "y_train_original = y_train.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample the training data using 10 random seeds, by using the `DataFrame.sample()` method with seeds ranging from 0 to 9.\n",
    "We then fit 10 models, each on one of the random splits, and evaluate their performance on the same validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = range(10)\n",
    "df_splits = pd.DataFrame(columns=['split',\n",
    "                                  'r2_train',\n",
    "                                  'mae_train',\n",
    "                                  'rmse_train',\n",
    "                                  'r2_val',\n",
    "                                  'mae_val',\n",
    "                                  'rmse_val'])\n",
    "\n",
    "for split in splits:\n",
    "    print(f'Fitting and evaluating random split {split}')\n",
    "    X_train = X_train_original.sample(frac=0.7, random_state=split)\n",
    "    y_train = y_train_original[X_train.index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = normalize(scaler.fit_transform(X_train))\n",
    "    X_val = normalize(scaler.transform(X_val_unscaled))\n",
    "    X_test = normalize(scaler.transform(X_test_unscaled))\n",
    "    \n",
    "    model = AdaBoostRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_act_val = y_val\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    r2_train, mae_train, rmse_train = evaluate_model(model, X_train, y_train)\n",
    "    r2_val, mae_val, rmse_val = evaluate_model(model, X_val, y_val)\n",
    "    result_dict = {\n",
    "        'split': split,\n",
    "        'r2_train': r2_train,\n",
    "        'mae_train': mae_train,\n",
    "        'rmse_train': rmse_train,\n",
    "        'r2_val': r2_val,\n",
    "        'mae_val': mae_val,\n",
    "        'rmse_val': rmse_val}\n",
    "    \n",
    "    df_splits = append_result_df(df_splits, result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splits['split'] = df_splits['split'].astype(int)\n",
    "df_splits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the train and validation $r^2$ scores for each of the 10 models.\n",
    "\n",
    "Note the high variability in the r2_val score. In contrast, the variability in the r2_train score is comparatively lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splits.plot('split', ['r2_train', 'r2_val'], kind='bar')\n",
    "plt.title(f'Performance of {type(model).__name__}\\nwith {len(splits)} different data splits')\n",
    "plt.ylim((0.5, 1.0))\n",
    "plt.ylabel('$r^2$')\n",
    "plt.xlabel('Split #')\n",
    "plt.legend(loc='lower right', framealpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This effect is even more pronounced when we plot the mean abolute error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splits.plot('split', ['mae_train', 'mae_val'], kind='bar')\n",
    "plt.title(f'Performance of {type(model).__name__}\\nwith {len(splits)} different data splits')\n",
    "plt.ylabel('MAE in $\\mathrm{C}_\\mathrm{p}$ (J / mol K)')\n",
    "plt.xlabel('Split #')\n",
    "plt.legend(loc='lower right', framealpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, typically the average value of all the scores are reported, as this gives a much more accurate estimate of how well the model actually performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_r2_val = df_splits['r2_val'].mean()\n",
    "avg_mae_val = df_splits['mae_val'].mean()\n",
    "\n",
    "print(f'Average validation r2: {avg_r2_val:0.4f}')\n",
    "print(f'Average validation MAE: {avg_mae_val:0.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course_book_II",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "3af2cce238079744ade4380fab4e623f56bf17adac44d2cac1f930dd9ee5f19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
