{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling using neural network / deep learning-based models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will cover how to implement a simple neural network for the modeling of heat capacity.\n",
    "\n",
    "We will load, prepare featurize, and scale/normalize the input datasets the same way as we did in the pervious notebook. For more information about the individual steps, you can consult that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from CBFV.cbfv.composition import generate_features\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Set a random seed to ensure reproducibility across runs\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizing and scaling data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing new here---same steps as we've done in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "train_path = os.path.join(PATH, 'data/cp_train.csv')\n",
    "val_path = os.path.join(PATH, 'data/cp_val.csv')\n",
    "test_path = os.path.join(PATH, 'data/cp_test.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_val = pd.read_csv(val_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(f'df_train DataFrame shape: {df_train.shape}')\n",
    "print(f'df_val DataFrame shape: {df_val.shape}')\n",
    "print(f'df_test DataFrame shape: {df_test.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do not sub-sample the datasets into smaller datasets like we did in the previous notebook.\n",
    "Typically, the more data you have for neural networks, the better the networks will be able to train, and the better they will perform (as long as they are well-conditioned).\n",
    "\n",
    "Additionally, the performance of `PyTorch` is very good for modern computers, especially if you have a modern CUDA-capable graphics processing unit (GPU) such as an Nvidia GPU to accelerate the computations.\n",
    "Our dataset is small enough to fit into almost all modern computers or CUDA-capable GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {'Cp': 'target'}\n",
    "df_train = df_train.rename(columns=rename_dict)\n",
    "df_val = df_val.rename(columns=rename_dict)\n",
    "df_test = df_test.rename(columns=rename_dict)\n",
    "\n",
    "X_train_unscaled, y_train, formulae_train, skipped_train = generate_features(df_train, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_val_unscaled, y_val, formulae_val, skipped_val = generate_features(df_val, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test_unscaled, y_test, formulae_test, skipped_test = generate_features(df_test, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train_unscaled)\n",
    "X_val = scaler.transform(X_val_unscaled)\n",
    "X_test = scaler.transform(X_test_unscaled)\n",
    "\n",
    "X_train = normalize(X_train)\n",
    "X_val = normalize(X_val)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where you get to be the architect, and design your own neural network!\n",
    "\n",
    "For sake of clarity (and to ensure that this tutorial runs on all the potatoes of this world), we will define a simple dense fully-connected neural network (which we will call `DenseNet`) as an example.\n",
    "\n",
    "The input layer of `DenseNet` accepts input data in the dimension of each row of the input data, which is equal to the number of features in our CBFV featurization scheme.\n",
    "In our particular example, when featurized using the `oliynyk` featurizer, the input dimension is 177 (it is the second dimension when you view `X_train.shape`).\n",
    "\n",
    "The output layer dimension of `DenseNet` is 1, because we want to predict one value (heat capacity).\n",
    "\n",
    "In addition, `DenseNet` can have one or more \"hidden layers\" that are attached between the input and output layers. These can be any arbitrary dimensions $>1$ you want to choose."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This implements a dynamically-built dense fully-connected neural network\n",
    "    with leaky ReLU activation and optional dropout.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dims: int\n",
    "        Number of input features (required).\n",
    "    hidden_dims: list of ints\n",
    "        Number of hidden features, where each integer represents the number of\n",
    "        hidden features in each subsequent hidden linear layer (optional,\n",
    "        default=[64, 32]).\n",
    "    output_dims: int\n",
    "        Number of output features (optional, default=1).\n",
    "    dropout: float\n",
    "        the dropout value (optional, default=0.0).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dims,\n",
    "                 hidden_dims=[64, 32],\n",
    "                 output_dims=1,\n",
    "                 dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dims = output_dims\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Build a sub-block of linear networks\n",
    "        def fc_block(in_dim, out_dim, *args, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(in_dim, out_dim, *args, **kwargs),\n",
    "                nn.Dropout(p=self.dropout),\n",
    "                nn.LeakyReLU()\n",
    "                )\n",
    "\n",
    "        # Build overall network architecture\n",
    "        self.network = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_dims, self.hidden_dims[0]),\n",
    "                    nn.Dropout(p=self.dropout),\n",
    "                    nn.LeakyReLU())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        hidden_layer_sizes = zip(self.hidden_dims[:-1], self.hidden_dims[1:])\n",
    "        self.network.extend([\n",
    "            fc_block(in_dim, out_dim) for in_dim, out_dim\n",
    "            in hidden_layer_sizes]\n",
    "            )\n",
    "\n",
    "        self.network.extend([\n",
    "            nn.Linear(hidden_dims[-1], output_dims)]\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DenseNet model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            A representation of the chemical compounds in the shape\n",
    "            (n_compounds, n_feats).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y: torch.Tensor\n",
    "            The element property prediction with the shape 1.\n",
    "        \"\"\"\n",
    "        for i, subnet in enumerate(self.network):\n",
    "            x = subnet(x)\n",
    "            \n",
    "        y = x\n",
    "\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the compute device for calculations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the neural network, we can first check if your machine has a CUDA-capable device.\n",
    "CUDA is a specific set of application instructions (application programming interfaces, APIs) that PyTorch can use to accelerate some of the calculations performed in neural networks.\n",
    "\n",
    "Generally, a relatively recent GPU from Nvidia will support CUDA capabilities, and can be used to accelerate neural network computations in PyTorch.\n",
    "\n",
    "In case you do not have a CUDA-capable device, PyTorch will fall back to using the CPU. Depending on the complexity of your model, training and predicting using a CPU can take significantly longer than using a CUDA-capable GPU.\n",
    "\n",
    "Consult the [PyTorch](https://pytorch.org/docs/stable/torch.html) and [CUDA](https://docs.nvidia.com/cuda/) documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_available = torch.cuda.is_available()\n",
    "print(f'CUDA is available: {CUDA_available}')\n",
    "\n",
    "if CUDA_available:\n",
    "    compute_device = torch.device('cuda')\n",
    "else:\n",
    "    compute_device = torch.device('cpu')\n",
    "    \n",
    "print(f'Compute device for PyTorch: {compute_device}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the data loader and dataset structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a dataloader class specific for loading CBFV-type datasets.\n",
    "\n",
    "We also define the CBFV dataset class that tells PyTorch how our dataset is structured, and how to grab individual data samples from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBFVDataLoader():\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: np.ndarray or pd.DataFrame or pd.Series\n",
    "        name of csv file containing cif and properties\n",
    "    val_data: np.ndarray or pd.DataFrame or pd.Series\n",
    "        name of csv file containing cif and properties\n",
    "    test_data: np.ndarray or pd.DataFrame or pd.Series\n",
    "        name of csv file containing cif and properties\n",
    "    batch_size: float, optional (default=64)\n",
    "        Step size for the Gaussian filter\n",
    "    random_state: int, optional (default=42)\n",
    "        Random seed for sampling the dataset. Only used if validation data is\n",
    "        not given.\n",
    "    shuffle: bool, optional (default=True)\n",
    "        Whether to shuffle the datasets or not\n",
    "    \"\"\"\n",
    "    def __init__(self, train_data, val_data, test_data,\n",
    "                 batch_size=64, num_workers=1, random_state=42,\n",
    "                 shuffle=True, pin_memory=True):\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "    def get_data_loaders(self, batch_size=1):\n",
    "        '''\n",
    "        Input the dataset, get train test split\n",
    "        '''\n",
    "        train_dataset = CBFVDataset(self.train_data)\n",
    "        val_dataset = CBFVDataset(self.val_data)\n",
    "        test_dataset = CBFVDataset(self.test_data)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=self.batch_size,\n",
    "                                  pin_memory=self.pin_memory,\n",
    "                                  shuffle=self.shuffle)\n",
    "\n",
    "        val_loader = DataLoader(val_dataset,\n",
    "                                batch_size=self.batch_size,\n",
    "                                pin_memory=self.pin_memory,\n",
    "                                shuffle=self.shuffle)\n",
    "\n",
    "        test_loader = DataLoader(test_dataset,\n",
    "                                 batch_size=self.batch_size,\n",
    "                                 pin_memory=self.pin_memory,\n",
    "                                 shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "class CBFVDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Get X and y from CBFV-based dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "        self.X = np.array(self.data[0])\n",
    "        self.y = np.array(self.data[1])\n",
    "        self.shape = [(self.X.shape), (self.y.shape)]\n",
    "\n",
    "    def __str__(self):\n",
    "        string = f'CBFVDataset with X.shape {self.X.shape}'\n",
    "        return string\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[[idx], :]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        X = torch.as_tensor(X)\n",
    "        y = torch.as_tensor(np.array(y))\n",
    "\n",
    "        return (X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose a batch size for loading data, and initialize the DataLoader for loading the featurized input data.\n",
    "\n",
    "We also get the data loaders corresponding to the train, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (X_train, y_train)\n",
    "val_data = (X_val, y_val)\n",
    "test_data = (X_test, y_test)\n",
    "\n",
    "# Instantiate the DataLoader\n",
    "batch_size = 128\n",
    "data_loaders = CBFVDataLoader(train_data, val_data, test_data, batch_size=batch_size)\n",
    "train_loader, val_loader, test_loader = data_loaders.get_data_loaders()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating a `DenseNet` model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can instantiate... an instance of the `DenseNet` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input dimension size from the dataset\n",
    "example_data = train_loader.dataset.data[0]\n",
    "input_dims = example_data.shape[-1]\n",
    "\n",
    "# Instantiate the model\n",
    "model = DenseNet(input_dims, hidden_dims=[16], dropout=0.0)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss criterion & optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see the model and its individual layers and components printed nicely.\n",
    "\n",
    "We then instantiate and initialize the loss criterion and optimizer.\n",
    "\n",
    "Note, there are many choices of loss criteria and optimizers that are provided by PyTorch, each with their benefits and limitations, and a myriad of parameters.\n",
    "Consult the PyTorch documentation for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss criterion\n",
    "criterion = nn.L1Loss()\n",
    "print('Loss criterion: ')\n",
    "print(criterion)\n",
    "\n",
    "# Initialize the optimzer\n",
    "optim_lr = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=optim_lr)\n",
    "print('\\nOptimizer: ')\n",
    "print(optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving the model to the compute device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can move the model and loss criterion computation to the compute device.\n",
    "\n",
    "If you have a GPU, this will trasnfer and attach the required resources to the GPU. If you have a CPU, then everything will remain on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model and criterion to the compute device\n",
    "model = model.to(compute_device)\n",
    "criterion = criterion.to(compute_device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some additional helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some scaler functions and helper functions to evaluate and visualize model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler():\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.as_tensor(data)\n",
    "        self.mean = torch.mean(self.data)\n",
    "        self.std = torch.std(self.data)\n",
    "\n",
    "    def scale(self, data):\n",
    "        data = torch.as_tensor(data)\n",
    "        data_scaled = (data - self.mean) / self.std\n",
    "        return data_scaled\n",
    "\n",
    "    def unscale(self, data_scaled):\n",
    "        data_scaled = torch.as_tensor(data_scaled)\n",
    "        data = data_scaled * self.std + self.mean\n",
    "        return data\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']\n",
    "\n",
    "\n",
    "class MeanLogNormScaler():\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.as_tensor(data)\n",
    "        self.logdata = torch.log(self.data)\n",
    "        self.mean = torch.mean(self.logdata)\n",
    "        self.std = torch.std(self.logdata)\n",
    "\n",
    "    def scale(self, data):\n",
    "        data = torch.as_tensor(data)\n",
    "        data_scaled = (torch.log(data) - self.mean) / self.std\n",
    "        return data_scaled\n",
    "\n",
    "    def unscale(self, data_scaled):\n",
    "        data_scaled = torch.as_tensor(data_scaled) * self.std + self.mean\n",
    "        data = torch.exp(data_scaled)\n",
    "        return data\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    target_list = []\n",
    "    pred_list = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data_output in enumerate(data_loader):\n",
    "            X, y_act = data_output\n",
    "            X = X.to(compute_device,\n",
    "                     dtype=data_type,\n",
    "                     non_blocking=True)\n",
    "            y_act = y_act.cpu().flatten().tolist()\n",
    "            y_pred = model.forward(X).cpu().flatten().tolist()\n",
    "\n",
    "            # Unscale target values\n",
    "            y_pred = target_scaler.unscale(y_pred).tolist()\n",
    "\n",
    "            targets = y_act\n",
    "            predictions = y_pred\n",
    "            target_list.extend(targets)\n",
    "            pred_list.extend(predictions)\n",
    "    model.train()\n",
    "\n",
    "    return target_list, pred_list\n",
    "\n",
    "\n",
    "def evaluate(target, pred):\n",
    "    r2 = r2_score(target, pred)\n",
    "    mae = mean_absolute_error(target, pred)\n",
    "    rmse = mean_squared_error(target, pred, squared=False)\n",
    "    output = (r2, mae, rmse)\n",
    "    return output\n",
    "\n",
    "\n",
    "def print_scores(scores, label=''):\n",
    "    r2, mae, rmse = scores\n",
    "    print(f'{label} r2: {r2:0.4f}')\n",
    "    print(f'{label} mae: {mae:0.4f}')\n",
    "    print(f'{label} rmse: {rmse:0.4f}')\n",
    "    return scores\n",
    "\n",
    "\n",
    "def plot_pred_act(act, pred, model, reg_line=True, label=''):\n",
    "    xy_max = np.max([np.max(act), np.max(pred)])\n",
    "\n",
    "    plot = plt.figure(figsize=(6,6))\n",
    "    plt.plot(act, pred, 'o', ms=9, mec='k', mfc='silver', alpha=0.4)\n",
    "    plt.plot([0, xy_max], [0, xy_max], 'k--', label='ideal')\n",
    "    if reg_line:\n",
    "        polyfit = np.polyfit(act, pred, deg=1)\n",
    "        reg_ys = np.poly1d(polyfit)(np.unique(act))\n",
    "        plt.plot(np.unique(act), reg_ys, alpha=0.8, label='linear fit')\n",
    "    plt.axis('scaled')\n",
    "    plt.xlabel(f'Actual {label}')\n",
    "    plt.ylabel(f'Predicted {label}')\n",
    "    plt.title(f'{type(model).__name__}, r2: {r2_score(act, pred):0.4f}')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [data[1].numpy().tolist() for data in train_loader]\n",
    "y_train = [item for sublist in y_train for item in sublist]\n",
    "\n",
    "y_train = train_loader.dataset.y\n",
    "\n",
    "target_scaler = MeanLogNormScaler(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we train the neural network.\n",
    "\n",
    "This is the training procedure for the neural network:\n",
    "* for each `epoch`: \n",
    "  * iterate through the train dataset using `train_loader`:\n",
    "    * scale the target data\n",
    "    * transfer input (`X`) and target (`y`) data to compute device\n",
    "    * reset the optimizer's gradient to zero\n",
    "    * compute the output of the model (forward pass)\n",
    "    * calculate the loss of the model (between the predicted and true target values)\n",
    "    * propagate the loss backwards through the model (backpropagation)\n",
    "    * update the weights throughout the model\n",
    "  * if `epoch == print_every`:\n",
    "    * print the current epoch (to keep track of training progress)\n",
    "  * if `epoch == plot_every`:\n",
    "    * evaluate the model on the validation dataset using `val_loader`\n",
    "    * plot predicted vs. actual value plots\n",
    "    * print the train and val $r^2$, $\\textrm{MAE}$ and $\\textrm{RMSE}$ scores of the model\n",
    "\n",
    "Note: training this network may take up to tens of minutes, depending on your hardware configuration and whether or not you have a CUDA-capable device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = torch.float\n",
    "epochs = 500\n",
    "\n",
    "print_every = 20\n",
    "plot_every = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if epoch % print_every == 0 or epoch == epochs - 1:\n",
    "        print(f'epoch: {epoch}')\n",
    "    if epoch % plot_every == 0:        \n",
    "        target_train, pred_train = predict(model, train_loader)\n",
    "        train_scores = evaluate(target_train, pred_train)\n",
    "        print_scores(train_scores, label='train')\n",
    "        \n",
    "        target_val, pred_val = predict(model, val_loader)\n",
    "        val_scores = evaluate(target_val, pred_val)\n",
    "        print_scores(val_scores, label='val')\n",
    "        plot_pred_act(target_val, pred_val, model, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')\n",
    "        plt.show()\n",
    "        \n",
    "    for i, data_output in enumerate(train_loader):\n",
    "        X, y = data_output\n",
    "        y = target_scaler.scale(y)\n",
    "        \n",
    "        X = X.to(compute_device,\n",
    "                 dtype=data_type,\n",
    "                 non_blocking=True)\n",
    "        y = y.to(compute_device,\n",
    "                 dtype=data_type,\n",
    "                 non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(X).flatten()\n",
    "        loss = criterion(output.view(-1), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with our trained neural network, we can evaluate the performance of the model (at the end of the training phase) on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_val, pred_val = predict(model, val_loader)\n",
    "scores = evaluate(target_val, pred_val)\n",
    "\n",
    "print_scores(scores, label='val')\n",
    "\n",
    "plot = plot_pred_act(target_val, pred_val, model, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping track of training progress -- avoid overfitting\n",
    "\n",
    "Note, you can keep track of the training progress by saving the train and validation metrics such as $r^2$ and MAE at every epoch.\n",
    "Then, you can plot so-called \"loss curves\" that show the loss of the model vs. epoch throughout the training process.\n",
    "This gives you additional insight into your model training process, and helps you diagnose issues such as overfitting, improper model/optimizer/loss parameters, and so on.\n",
    "\n",
    "Once you start tracking these performance metrics during your training loop, you can also implement more advanced training techniques such as \"early stopping\".\n",
    "In early stopping, you observe the performance metrics (such as validation $r^2$ or MAE) over the training epochs, and you stop the training process if you observe that the metrics are not improving any more (meaning your model is fully trained), or if the metrics are increasing again after reaching a minimum (meaning your model is overfitting the training set).\n",
    "\n",
    "## Evaluating model performance on test dataset\n",
    "\n",
    "And finally evaluate the performance on the test dataset.\n",
    "**Remember:** you should only do this *once!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test, pred_test = predict(model, test_loader)\n",
    "scores = evaluate(target_test, pred_test)\n",
    "\n",
    "print_scores(scores, label='test')\n",
    "\n",
    "plot = plot_pred_act(target_test, pred_test, model, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting PyTorch models\n",
    "\n",
    "Now that we've got a (reasonably well-performing) model, we can export the weights and biases from the model to what is referred to as a \"checkpoint\" file.\n",
    "\n",
    "The advantages of exporting your model to a checkpoint file are manifold.\n",
    "For example, when you want to re-use the model again later (to make further predictions, or even to continue training), you don't have to train the model from scratch again.\n",
    "For our current `DenseNet` model, this may not seem like a big deal, since it trains within minutes.\n",
    "But once you start moving on to larger and larger models, model training time can reach hours, days---even weeks!\n",
    "\n",
    "Another advantage is that you can greatly enhance the reproducibility of your work.\n",
    "If you export your models, other researchers can then recreate your model architecture on their system, then load your weights into the model to get exactly the model you trained.\n",
    "This allows them to use your model as-is, and enables them to reproduce your work---an important step if they are to judge the merit of your work.\n",
    "\n",
    "With that said, we will now use PyTorch's built-in methods to export (1) our `DenseNet` model, and (2) our `target_scaler` (we need to export our `target_scaler` object as well, because we need to use it to unscale the model predictions to get back the true prediction values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model + target scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict = {'weights': model.state_dict(),\n",
    "             'scaler_state': target_scaler.state_dict()}\n",
    "print(save_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_path = ('model_checkpoint.pth') # .pth is commonly used as the file extension\n",
    "torch.save(save_dict, pth_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to your notebooks directory. You should now find a file named 'model_checkpoint.pth'.\n",
    "Since the `DenseNet` model is small, the checkpoint file weighs in relatively lightly at 13KB.\n",
    "Bigger models will have more weights & biases, and will require more storage space for the checkpoint file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model + target scaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if you provide the facilities to **save** a model, you should also provide facilities to **load** them and to recreate your model back.\n",
    "\n",
    "Thankfully, PyTorch makes this also easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, clear the variables for model and target_scaler.\n",
    "# We want to start with a clean slate.\n",
    "model = None\n",
    "target_scaler = None\n",
    "del model\n",
    "del target_scaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by recreating the `DenseNet` model and the `target_scaler` that we originally built. This model will be initialized with random weights & biases, which we will then overload (overwrite) afterwards with the values from the checkpoint file.\n",
    "\n",
    "Make sure that you create the same model and `target_scaler` here as the ones you saved the checkpoint file from. Otherwise you will not be able to load the checkpoint file, or it will produce unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model.\n",
    "# The model will be randomly initialized, but we will overwrite\n",
    "# all weights and biases when we load the checkpoint.\n",
    "model = DenseNet(input_dims, hidden_dims=[16], dropout=0.0)\n",
    "model = model.to(compute_device)\n",
    "print(model)\n",
    "\n",
    "# Instantiate the target_scaler.\n",
    "# We initialize this target_scaler with a vector of zeros,\n",
    "# but we will overwrite its internal parameters\n",
    "# when we load the checkpoint.\n",
    "target_scaler = MeanLogNormScaler(torch.zeros(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint and map it to the compute device\n",
    "pth_path = ('model_checkpoint.pth')\n",
    "checkpoint = torch.load(pth_path, map_location=compute_device)\n",
    "\n",
    "# Load the state dictionaries back into the model and target_scaler\n",
    "model.load_state_dict(checkpoint['weights'])\n",
    "target_scaler.load_state_dict(checkpoint['scaler_state'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test, pred_test = predict(model, test_loader)\n",
    "scores = evaluate(target_test, pred_test)\n",
    "\n",
    "print_scores(scores, label='test')\n",
    "\n",
    "plot = plot_pred_act(target_test, pred_test, model, label='$\\mathrm{C}_\\mathrm{p}$ (J / mol K)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course_book_II",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:28:38) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3af2cce238079744ade4380fab4e623f56bf17adac44d2cac1f930dd9ee5f19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
